Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.
Some model parameters or buffers are not found in the checkpoint:
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}
[34mroi_heads.box_predictor.cls_score.{bias, weight}
[32m[03/25 06:50:57 d2.engine.defaults]: [39mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=11, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)
    )
  )
)
[32m[03/25 06:50:58 d2.data.datasets.coco]: [39mLoaded 4883 images in COCO format from ../../dataset/train.json
[32m[03/25 06:50:58 d2.data.build]: [39mRemoved 0 images with no usable annotations. 4883 images left.
[32m[03/25 06:50:58 d2.data.build]: [39mDistribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
[36m|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
[36m| General trash | 3966         |    Paper    | 6352         | Paper pack | 897          |
[36m|     Metal     | 936          |    Glass    | 982          |  Plastic   | 2943         |
[36m|   Styrofoam   | 1263         | Plastic bag | 5178         |  Battery   | 159          |
[36m|   Clothing    | 468          |             |              |            |              |
[36m|     total     | 23144        |             |              |            |              |
[32m[03/25 06:50:58 d2.data.build]: [39mUsing training sampler TrainingSampler
[32m[03/25 06:50:58 d2.data.common]: [39mSerializing 4883 elements to byte tensors and concatenating them all ...
[32m[03/25 06:50:58 d2.data.common]: [39mSerialized dataset takes 2.19 MiB
expected_results:
[]
[32m[03/25 06:50:58 d2.engine.train_loop]: [39mStarting training from iteration 0
/opt/ml/detection/baseline/detectron2/detectron2/modeling/roi_heads/fast_rcnn.py:103: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  num_fg = fg_inds.nonzero().numel()
[32m[03/25 06:51:11 d2.utils.events]: [39m eta: 2:25:29  iter: 19  total_loss: 3.208  loss_cls: 2.398  loss_box_reg: 0.7046  loss_rpn_cls: 0.04939  loss_rpn_loc: 0.0208  time: 0.5832  data_time: 0.0293  lr: 1.9981e-05  max_mem: 6307M
[32m[03/25 06:51:23 d2.utils.events]: [39m eta: 2:25:14  iter: 39  total_loss: 2.833  loss_cls: 1.967  loss_box_reg: 0.6748  loss_rpn_cls: 0.06374  loss_rpn_loc: 0.03402  time: 0.5834  data_time: 0.0124  lr: 3.9961e-05  max_mem: 6307M
[32m[03/25 06:51:34 d2.utils.events]: [39m eta: 2:24:21  iter: 59  total_loss: 2.047  loss_cls: 1.225  loss_box_reg: 0.7131  loss_rpn_cls: 0.07913  loss_rpn_loc: 0.0332  time: 0.5820  data_time: 0.0118  lr: 5.9941e-05  max_mem: 6307M
[32m[03/25 06:51:46 d2.utils.events]: [39m eta: 2:24:09  iter: 79  total_loss: 1.564  loss_cls: 0.7866  loss_box_reg: 0.6524  loss_rpn_cls: 0.0438  loss_rpn_loc: 0.02031  time: 0.5818  data_time: 0.0117  lr: 7.9921e-05  max_mem: 6307M
[32m[03/25 06:51:58 d2.utils.events]: [39m eta: 2:24:08  iter: 99  total_loss: 1.754  loss_cls: 0.8016  loss_box_reg: 0.7274  loss_rpn_cls: 0.08548  loss_rpn_loc: 0.03635  time: 0.5824  data_time: 0.0129  lr: 9.9901e-05  max_mem: 6307M
[32m[03/25 06:52:09 d2.utils.events]: [39m eta: 2:23:58  iter: 119  total_loss: 1.574  loss_cls: 0.7212  loss_box_reg: 0.7406  loss_rpn_cls: 0.05153  loss_rpn_loc: 0.02522  time: 0.5825  data_time: 0.0124  lr: 0.00011988  max_mem: 6307M
[32m[03/25 06:52:21 d2.utils.events]: [39m eta: 2:23:43  iter: 139  total_loss: 1.493  loss_cls: 0.6911  loss_box_reg: 0.71  loss_rpn_cls: 0.04093  loss_rpn_loc: 0.03634  time: 0.5824  data_time: 0.0123  lr: 0.00013986  max_mem: 6307M
[32m[03/25 06:52:33 d2.utils.events]: [39m eta: 2:23:31  iter: 159  total_loss: 1.514  loss_cls: 0.6846  loss_box_reg: 0.7176  loss_rpn_cls: 0.02431  loss_rpn_loc: 0.02979  time: 0.5826  data_time: 0.0126  lr: 0.00015984  max_mem: 6307M
[32m[03/25 06:52:44 d2.utils.events]: [39m eta: 2:23:24  iter: 179  total_loss: 1.407  loss_cls: 0.6593  loss_box_reg: 0.7113  loss_rpn_cls: 0.02863  loss_rpn_loc: 0.01672  time: 0.5831  data_time: 0.0129  lr: 0.00017982  max_mem: 6307M
[32m[03/25 06:52:56 d2.utils.events]: [39m eta: 2:23:15  iter: 199  total_loss: 1.372  loss_cls: 0.6229  loss_box_reg: 0.6559  loss_rpn_cls: 0.04386  loss_rpn_loc: 0.02705  time: 0.5832  data_time: 0.0126  lr: 0.0001998  max_mem: 6307M
[32m[03/25 06:53:08 d2.utils.events]: [39m eta: 2:23:07  iter: 219  total_loss: 1.502  loss_cls: 0.6366  loss_box_reg: 0.7043  loss_rpn_cls: 0.04089  loss_rpn_loc: 0.02437  time: 0.5837  data_time: 0.0146  lr: 0.00021978  max_mem: 6307M
[32m[03/25 06:53:20 d2.utils.events]: [39m eta: 2:22:54  iter: 239  total_loss: 1.275  loss_cls: 0.5674  loss_box_reg: 0.6652  loss_rpn_cls: 0.02666  loss_rpn_loc: 0.01678  time: 0.5835  data_time: 0.0124  lr: 0.00023976  max_mem: 6307M
[32m[03/25 06:53:31 d2.utils.events]: [39m eta: 2:22:42  iter: 259  total_loss: 1.376  loss_cls: 0.6062  loss_box_reg: 0.6929  loss_rpn_cls: 0.03577  loss_rpn_loc: 0.03324  time: 0.5832  data_time: 0.0123  lr: 0.00025974  max_mem: 6307M
[32m[03/25 06:53:43 d2.utils.events]: [39m eta: 2:22:27  iter: 279  total_loss: 1.226  loss_cls: 0.5521  loss_box_reg: 0.6256  loss_rpn_cls: 0.02841  loss_rpn_loc: 0.03148  time: 0.5833  data_time: 0.0165  lr: 0.00027972  max_mem: 6307M
[32m[03/25 06:53:54 d2.utils.events]: [39m eta: 2:22:14  iter: 299  total_loss: 1.307  loss_cls: 0.6066  loss_box_reg: 0.6586  loss_rpn_cls: 0.02677  loss_rpn_loc: 0.03087  time: 0.5832  data_time: 0.0122  lr: 0.0002997  max_mem: 6307M
[32m[03/25 06:54:06 d2.utils.events]: [39m eta: 2:22:03  iter: 319  total_loss: 1.225  loss_cls: 0.5793  loss_box_reg: 0.5801  loss_rpn_cls: 0.03476  loss_rpn_loc: 0.03194  time: 0.5830  data_time: 0.0122  lr: 0.00031968  max_mem: 6307M
[32m[03/25 06:54:18 d2.utils.events]: [39m eta: 2:21:53  iter: 339  total_loss: 1.185  loss_cls: 0.5405  loss_box_reg: 0.5474  loss_rpn_cls: 0.02874  loss_rpn_loc: 0.02255  time: 0.5832  data_time: 0.0136  lr: 0.00033966  max_mem: 6307M
[32m[03/25 06:54:29 d2.utils.events]: [39m eta: 2:21:41  iter: 359  total_loss: 1.355  loss_cls: 0.6413  loss_box_reg: 0.5715  loss_rpn_cls: 0.03567  loss_rpn_loc: 0.03805  time: 0.5831  data_time: 0.0131  lr: 0.00035964  max_mem: 6307M
[32m[03/25 06:54:41 d2.utils.events]: [39m eta: 2:21:29  iter: 379  total_loss: 1.111  loss_cls: 0.5428  loss_box_reg: 0.4748  loss_rpn_cls: 0.03252  loss_rpn_loc: 0.02477  time: 0.5831  data_time: 0.0134  lr: 0.00037962  max_mem: 6307M
[32m[03/25 06:54:53 d2.utils.events]: [39m eta: 2:21:18  iter: 399  total_loss: 0.9416  loss_cls: 0.5104  loss_box_reg: 0.3939  loss_rpn_cls: 0.02196  loss_rpn_loc: 0.01523  time: 0.5833  data_time: 0.0128  lr: 0.0003996  max_mem: 6307M
[32m[03/25 06:55:05 d2.utils.events]: [39m eta: 2:21:05  iter: 419  total_loss: 1.063  loss_cls: 0.5438  loss_box_reg: 0.5058  loss_rpn_cls: 0.03001  loss_rpn_loc: 0.02599  time: 0.5831  data_time: 0.0122  lr: 0.00041958  max_mem: 6307M
[32m[03/25 06:55:16 d2.utils.events]: [39m eta: 2:20:53  iter: 439  total_loss: 1.018  loss_cls: 0.5295  loss_box_reg: 0.4163  loss_rpn_cls: 0.03345  loss_rpn_loc: 0.02343  time: 0.5831  data_time: 0.0126  lr: 0.00043956  max_mem: 6307M
[32m[03/25 06:55:28 d2.utils.events]: [39m eta: 2:20:41  iter: 459  total_loss: 1.023  loss_cls: 0.5563  loss_box_reg: 0.4063  loss_rpn_cls: 0.03678  loss_rpn_loc: 0.02907  time: 0.5829  data_time: 0.0123  lr: 0.00045954  max_mem: 6307M
[32m[03/25 06:55:39 d2.utils.events]: [39m eta: 2:20:30  iter: 479  total_loss: 1.023  loss_cls: 0.554  loss_box_reg: 0.4048  loss_rpn_cls: 0.03736  loss_rpn_loc: 0.03463  time: 0.5830  data_time: 0.0126  lr: 0.00047952  max_mem: 6307M
[32m[03/25 06:55:51 d2.utils.events]: [39m eta: 2:20:18  iter: 499  total_loss: 1.076  loss_cls: 0.5613  loss_box_reg: 0.4334  loss_rpn_cls: 0.04622  loss_rpn_loc: 0.04014  time: 0.5829  data_time: 0.0130  lr: 0.0004995  max_mem: 6307M
[32m[03/25 06:56:03 d2.utils.events]: [39m eta: 2:20:07  iter: 519  total_loss: 0.818  loss_cls: 0.4638  loss_box_reg: 0.336  loss_rpn_cls: 0.0195  loss_rpn_loc: 0.02498  time: 0.5828  data_time: 0.0125  lr: 0.00051948  max_mem: 6307M
[32m[03/25 06:56:14 d2.utils.events]: [39m eta: 2:19:55  iter: 539  total_loss: 1.051  loss_cls: 0.5405  loss_box_reg: 0.427  loss_rpn_cls: 0.03108  loss_rpn_loc: 0.02763  time: 0.5828  data_time: 0.0125  lr: 0.00053946  max_mem: 6307M
[32m[03/25 06:56:26 d2.utils.events]: [39m eta: 2:19:43  iter: 559  total_loss: 1.034  loss_cls: 0.4844  loss_box_reg: 0.4018  loss_rpn_cls: 0.03242  loss_rpn_loc: 0.03133  time: 0.5828  data_time: 0.0133  lr: 0.00055944  max_mem: 6307M
[32m[03/25 06:56:38 d2.utils.events]: [39m eta: 2:19:31  iter: 579  total_loss: 0.9681  loss_cls: 0.5333  loss_box_reg: 0.4067  loss_rpn_cls: 0.02475  loss_rpn_loc: 0.02459  time: 0.5827  data_time: 0.0136  lr: 0.00057942  max_mem: 6307M
[32m[03/25 06:56:49 d2.utils.events]: [39m eta: 2:19:20  iter: 599  total_loss: 0.9835  loss_cls: 0.5051  loss_box_reg: 0.393  loss_rpn_cls: 0.02397  loss_rpn_loc: 0.03035  time: 0.5828  data_time: 0.0131  lr: 0.0005994  max_mem: 6307M
[32m[03/25 06:57:01 d2.utils.events]: [39m eta: 2:19:09  iter: 619  total_loss: 0.8369  loss_cls: 0.4658  loss_box_reg: 0.2866  loss_rpn_cls: 0.02424  loss_rpn_loc: 0.01939  time: 0.5829  data_time: 0.0154  lr: 0.00061938  max_mem: 6307M
[32m[03/25 06:57:13 d2.utils.events]: [39m eta: 2:18:58  iter: 639  total_loss: 0.9392  loss_cls: 0.4748  loss_box_reg: 0.347  loss_rpn_cls: 0.03109  loss_rpn_loc: 0.03632  time: 0.5829  data_time: 0.0135  lr: 0.00063936  max_mem: 6307M
[32m[03/25 06:57:25 d2.utils.events]: [39m eta: 2:18:47  iter: 659  total_loss: 0.8471  loss_cls: 0.4455  loss_box_reg: 0.3507  loss_rpn_cls: 0.03367  loss_rpn_loc: 0.02267  time: 0.5830  data_time: 0.0139  lr: 0.00065934  max_mem: 6307M
[32m[03/25 06:57:36 d2.utils.events]: [39m eta: 2:18:35  iter: 679  total_loss: 0.913  loss_cls: 0.4835  loss_box_reg: 0.2954  loss_rpn_cls: 0.03337  loss_rpn_loc: 0.02695  time: 0.5829  data_time: 0.0125  lr: 0.00067932  max_mem: 6307M
[32m[03/25 06:57:48 d2.utils.events]: [39m eta: 2:18:25  iter: 699  total_loss: 0.8437  loss_cls: 0.4366  loss_box_reg: 0.3249  loss_rpn_cls: 0.03414  loss_rpn_loc: 0.02094  time: 0.5833  data_time: 0.0160  lr: 0.0006993  max_mem: 6307M
[32m[03/25 06:58:00 d2.utils.events]: [39m eta: 2:18:14  iter: 719  total_loss: 1.01  loss_cls: 0.5416  loss_box_reg: 0.3699  loss_rpn_cls: 0.03286  loss_rpn_loc: 0.02618  time: 0.5833  data_time: 0.0129  lr: 0.00071928  max_mem: 6307M
[32m[03/25 06:58:12 d2.utils.events]: [39m eta: 2:18:03  iter: 739  total_loss: 0.8504  loss_cls: 0.4893  loss_box_reg: 0.2916  loss_rpn_cls: 0.02908  loss_rpn_loc: 0.03039  time: 0.5833  data_time: 0.0126  lr: 0.00073926  max_mem: 6307M
[32m[03/25 06:58:23 d2.utils.events]: [39m eta: 2:17:51  iter: 759  total_loss: 0.8656  loss_cls: 0.4751  loss_box_reg: 0.3413  loss_rpn_cls: 0.02309  loss_rpn_loc: 0.02127  time: 0.5833  data_time: 0.0128  lr: 0.00075924  max_mem: 6307M
[32m[03/25 06:58:35 d2.utils.events]: [39m eta: 2:17:39  iter: 779  total_loss: 1.051  loss_cls: 0.4989  loss_box_reg: 0.4355  loss_rpn_cls: 0.02582  loss_rpn_loc: 0.03791  time: 0.5832  data_time: 0.0132  lr: 0.00077922  max_mem: 6307M
[32m[03/25 06:58:46 d2.utils.events]: [39m eta: 2:17:27  iter: 799  total_loss: 0.935  loss_cls: 0.484  loss_box_reg: 0.3632  loss_rpn_cls: 0.02972  loss_rpn_loc: 0.03839  time: 0.5831  data_time: 0.0125  lr: 0.0007992  max_mem: 6307M
[32m[03/25 06:58:58 d2.utils.events]: [39m eta: 2:17:15  iter: 819  total_loss: 0.7092  loss_cls: 0.4331  loss_box_reg: 0.2669  loss_rpn_cls: 0.01382  loss_rpn_loc: 0.0155  time: 0.5831  data_time: 0.0125  lr: 0.00081918  max_mem: 6307M
[32m[03/25 06:59:10 d2.utils.events]: [39m eta: 2:17:04  iter: 839  total_loss: 0.9519  loss_cls: 0.4979  loss_box_reg: 0.3424  loss_rpn_cls: 0.03156  loss_rpn_loc: 0.02897  time: 0.5830  data_time: 0.0131  lr: 0.00083916  max_mem: 6307M
[32m[03/25 06:59:21 d2.utils.events]: [39m eta: 2:16:52  iter: 859  total_loss: 0.9053  loss_cls: 0.5261  loss_box_reg: 0.322  loss_rpn_cls: 0.02764  loss_rpn_loc: 0.02998  time: 0.5830  data_time: 0.0132  lr: 0.00085914  max_mem: 6307M
[32m[03/25 06:59:33 d2.utils.events]: [39m eta: 2:16:40  iter: 879  total_loss: 0.9124  loss_cls: 0.5254  loss_box_reg: 0.3175  loss_rpn_cls: 0.02215  loss_rpn_loc: 0.02299  time: 0.5829  data_time: 0.0127  lr: 0.00087912  max_mem: 6307M
[32m[03/25 06:59:45 d2.utils.events]: [39m eta: 2:16:29  iter: 899  total_loss: 0.8299  loss_cls: 0.4629  loss_box_reg: 0.3349  loss_rpn_cls: 0.01676  loss_rpn_loc: 0.0303  time: 0.5829  data_time: 0.0130  lr: 0.0008991  max_mem: 6307M
[32m[03/25 06:59:56 d2.utils.events]: [39m eta: 2:16:16  iter: 919  total_loss: 0.9128  loss_cls: 0.4935  loss_box_reg: 0.3627  loss_rpn_cls: 0.0263  loss_rpn_loc: 0.0265  time: 0.5828  data_time: 0.0126  lr: 0.00091908  max_mem: 6307M
[32m[03/25 07:00:08 d2.utils.events]: [39m eta: 2:16:05  iter: 939  total_loss: 0.898  loss_cls: 0.4377  loss_box_reg: 0.3647  loss_rpn_cls: 0.02838  loss_rpn_loc: 0.03232  time: 0.5828  data_time: 0.0129  lr: 0.00093906  max_mem: 6307M
[32m[03/25 07:00:19 d2.utils.events]: [39m eta: 2:15:53  iter: 959  total_loss: 0.8627  loss_cls: 0.4824  loss_box_reg: 0.3121  loss_rpn_cls: 0.02589  loss_rpn_loc: 0.02753  time: 0.5827  data_time: 0.0128  lr: 0.00095904  max_mem: 6307M
[32m[03/25 07:00:31 d2.utils.events]: [39m eta: 2:15:41  iter: 979  total_loss: 0.9237  loss_cls: 0.5263  loss_box_reg: 0.3216  loss_rpn_cls: 0.03042  loss_rpn_loc: 0.02327  time: 0.5827  data_time: 0.0126  lr: 0.00097902  max_mem: 6307M
[32m[03/25 07:00:43 d2.utils.events]: [39m eta: 2:15:29  iter: 999  total_loss: 0.7782  loss_cls: 0.4474  loss_box_reg: 0.2736  loss_rpn_cls: 0.03864  loss_rpn_loc: 0.04377  time: 0.5826  data_time: 0.0131  lr: 0.000999  max_mem: 6307M
[32m[03/25 07:00:54 d2.utils.events]: [39m eta: 2:15:17  iter: 1019  total_loss: 0.8026  loss_cls: 0.4224  loss_box_reg: 0.3433  loss_rpn_cls: 0.02598  loss_rpn_loc: 0.03207  time: 0.5826  data_time: 0.0129  lr: 0.001  max_mem: 6307M
[32m[03/25 07:01:06 d2.utils.events]: [39m eta: 2:15:05  iter: 1039  total_loss: 0.8227  loss_cls: 0.4828  loss_box_reg: 0.3165  loss_rpn_cls: 0.0242  loss_rpn_loc: 0.02543  time: 0.5826  data_time: 0.0138  lr: 0.001  max_mem: 6307M
[32m[03/25 07:01:18 d2.utils.events]: [39m eta: 2:14:54  iter: 1059  total_loss: 0.8367  loss_cls: 0.4678  loss_box_reg: 0.2874  loss_rpn_cls: 0.01905  loss_rpn_loc: 0.02318  time: 0.5825  data_time: 0.0129  lr: 0.001  max_mem: 6307M
[32m[03/25 07:01:29 d2.utils.events]: [39m eta: 2:14:42  iter: 1079  total_loss: 0.8652  loss_cls: 0.4515  loss_box_reg: 0.3618  loss_rpn_cls: 0.0202  loss_rpn_loc: 0.02692  time: 0.5824  data_time: 0.0128  lr: 0.001  max_mem: 6307M
[32m[03/25 07:01:41 d2.utils.events]: [39m eta: 2:14:29  iter: 1099  total_loss: 0.7952  loss_cls: 0.4204  loss_box_reg: 0.3487  loss_rpn_cls: 0.02091  loss_rpn_loc: 0.01593  time: 0.5824  data_time: 0.0131  lr: 0.001  max_mem: 6307M
[32m[03/25 07:01:52 d2.utils.events]: [39m eta: 2:14:17  iter: 1119  total_loss: 0.7563  loss_cls: 0.4317  loss_box_reg: 0.2582  loss_rpn_cls: 0.01837  loss_rpn_loc: 0.01794  time: 0.5824  data_time: 0.0127  lr: 0.001  max_mem: 6307M
[32m[03/25 07:02:04 d2.utils.events]: [39m eta: 2:14:07  iter: 1139  total_loss: 0.8094  loss_cls: 0.4351  loss_box_reg: 0.3119  loss_rpn_cls: 0.02508  loss_rpn_loc: 0.02708  time: 0.5824  data_time: 0.0143  lr: 0.001  max_mem: 6307M
[32m[03/25 07:02:16 d2.utils.events]: [39m eta: 2:13:55  iter: 1159  total_loss: 0.7955  loss_cls: 0.448  loss_box_reg: 0.299  loss_rpn_cls: 0.02564  loss_rpn_loc: 0.02179  time: 0.5825  data_time: 0.0140  lr: 0.001  max_mem: 6307M
[32m[03/25 07:02:28 d2.utils.events]: [39m eta: 2:13:44  iter: 1179  total_loss: 0.9049  loss_cls: 0.5167  loss_box_reg: 0.3212  loss_rpn_cls: 0.03076  loss_rpn_loc: 0.02673  time: 0.5825  data_time: 0.0138  lr: 0.001  max_mem: 6307M
[32m[03/25 07:02:39 d2.utils.events]: [39m eta: 2:13:31  iter: 1199  total_loss: 0.7879  loss_cls: 0.4273  loss_box_reg: 0.2917  loss_rpn_cls: 0.02327  loss_rpn_loc: 0.02007  time: 0.5825  data_time: 0.0131  lr: 0.001  max_mem: 6307M
[32m[03/25 07:02:51 d2.utils.events]: [39m eta: 2:13:19  iter: 1219  total_loss: 0.8965  loss_cls: 0.5081  loss_box_reg: 0.3158  loss_rpn_cls: 0.02699  loss_rpn_loc: 0.0237  time: 0.5825  data_time: 0.0130  lr: 0.001  max_mem: 6307M
[32m[03/25 07:03:03 d2.utils.events]: [39m eta: 2:13:08  iter: 1239  total_loss: 0.7849  loss_cls: 0.4473  loss_box_reg: 0.2824  loss_rpn_cls: 0.02813  loss_rpn_loc: 0.02596  time: 0.5825  data_time: 0.0134  lr: 0.001  max_mem: 6307M
[32m[03/25 07:03:14 d2.utils.events]: [39m eta: 2:12:57  iter: 1259  total_loss: 0.6559  loss_cls: 0.3705  loss_box_reg: 0.261  loss_rpn_cls: 0.01557  loss_rpn_loc: 0.01546  time: 0.5825  data_time: 0.0140  lr: 0.001  max_mem: 6307M
[32m[03/25 07:03:26 d2.utils.events]: [39m eta: 2:12:46  iter: 1279  total_loss: 0.8622  loss_cls: 0.4697  loss_box_reg: 0.325  loss_rpn_cls: 0.03217  loss_rpn_loc: 0.03283  time: 0.5825  data_time: 0.0139  lr: 0.001  max_mem: 6307M
[32m[03/25 07:03:38 d2.utils.events]: [39m eta: 2:12:34  iter: 1299  total_loss: 0.8106  loss_cls: 0.4531  loss_box_reg: 0.3142  loss_rpn_cls: 0.02339  loss_rpn_loc: 0.02856  time: 0.5826  data_time: 0.0142  lr: 0.001  max_mem: 6307M
[32m[03/25 07:03:49 d2.utils.events]: [39m eta: 2:12:23  iter: 1319  total_loss: 0.8552  loss_cls: 0.4789  loss_box_reg: 0.3049  loss_rpn_cls: 0.02939  loss_rpn_loc: 0.03272  time: 0.5825  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:04:01 d2.utils.events]: [39m eta: 2:12:11  iter: 1339  total_loss: 0.821  loss_cls: 0.4465  loss_box_reg: 0.2926  loss_rpn_cls: 0.02816  loss_rpn_loc: 0.04832  time: 0.5825  data_time: 0.0127  lr: 0.001  max_mem: 6307M
[32m[03/25 07:04:13 d2.utils.events]: [39m eta: 2:12:00  iter: 1359  total_loss: 0.7531  loss_cls: 0.4048  loss_box_reg: 0.2853  loss_rpn_cls: 0.02417  loss_rpn_loc: 0.02755  time: 0.5826  data_time: 0.0131  lr: 0.001  max_mem: 6307M
[32m[03/25 07:04:24 d2.utils.events]: [39m eta: 2:11:48  iter: 1379  total_loss: 0.848  loss_cls: 0.4269  loss_box_reg: 0.2954  loss_rpn_cls: 0.03728  loss_rpn_loc: 0.03278  time: 0.5826  data_time: 0.0129  lr: 0.001  max_mem: 6307M
[32m[03/25 07:04:36 d2.utils.events]: [39m eta: 2:11:38  iter: 1399  total_loss: 0.8521  loss_cls: 0.4351  loss_box_reg: 0.3098  loss_rpn_cls: 0.0226  loss_rpn_loc: 0.02792  time: 0.5826  data_time: 0.0155  lr: 0.001  max_mem: 6307M
[32m[03/25 07:04:48 d2.utils.events]: [39m eta: 2:11:26  iter: 1419  total_loss: 0.7022  loss_cls: 0.4099  loss_box_reg: 0.2469  loss_rpn_cls: 0.02745  loss_rpn_loc: 0.01824  time: 0.5827  data_time: 0.0138  lr: 0.001  max_mem: 6307M
[32m[03/25 07:04:59 d2.utils.events]: [39m eta: 2:11:15  iter: 1439  total_loss: 0.831  loss_cls: 0.4357  loss_box_reg: 0.3219  loss_rpn_cls: 0.03875  loss_rpn_loc: 0.02858  time: 0.5827  data_time: 0.0124  lr: 0.001  max_mem: 6307M
[32m[03/25 07:05:11 d2.utils.events]: [39m eta: 2:11:03  iter: 1459  total_loss: 0.853  loss_cls: 0.4676  loss_box_reg: 0.348  loss_rpn_cls: 0.02496  loss_rpn_loc: 0.02259  time: 0.5826  data_time: 0.0126  lr: 0.001  max_mem: 6307M
[32m[03/25 07:05:23 d2.utils.events]: [39m eta: 2:10:52  iter: 1479  total_loss: 0.8499  loss_cls: 0.4235  loss_box_reg: 0.3239  loss_rpn_cls: 0.02589  loss_rpn_loc: 0.04075  time: 0.5827  data_time: 0.0155  lr: 0.001  max_mem: 6307M
[32m[03/25 07:05:34 d2.utils.events]: [39m eta: 2:10:40  iter: 1499  total_loss: 0.7221  loss_cls: 0.4222  loss_box_reg: 0.249  loss_rpn_cls: 0.01616  loss_rpn_loc: 0.01373  time: 0.5827  data_time: 0.0141  lr: 0.001  max_mem: 6307M
[32m[03/25 07:05:46 d2.utils.events]: [39m eta: 2:10:29  iter: 1519  total_loss: 0.8074  loss_cls: 0.4264  loss_box_reg: 0.3004  loss_rpn_cls: 0.02599  loss_rpn_loc: 0.03831  time: 0.5827  data_time: 0.0137  lr: 0.001  max_mem: 6307M
[32m[03/25 07:05:58 d2.utils.events]: [39m eta: 2:10:17  iter: 1539  total_loss: 0.7908  loss_cls: 0.4186  loss_box_reg: 0.2678  loss_rpn_cls: 0.01338  loss_rpn_loc: 0.01662  time: 0.5827  data_time: 0.0126  lr: 0.001  max_mem: 6307M
[32m[03/25 07:06:09 d2.utils.events]: [39m eta: 2:10:06  iter: 1559  total_loss: 0.8435  loss_cls: 0.4524  loss_box_reg: 0.3091  loss_rpn_cls: 0.02297  loss_rpn_loc: 0.02436  time: 0.5827  data_time: 0.0130  lr: 0.001  max_mem: 6307M
[32m[03/25 07:06:21 d2.utils.events]: [39m eta: 2:09:55  iter: 1579  total_loss: 0.9643  loss_cls: 0.4782  loss_box_reg: 0.3509  loss_rpn_cls: 0.03507  loss_rpn_loc: 0.04431  time: 0.5827  data_time: 0.0133  lr: 0.001  max_mem: 6307M
[32m[03/25 07:06:33 d2.utils.events]: [39m eta: 2:09:43  iter: 1599  total_loss: 0.7465  loss_cls: 0.3991  loss_box_reg: 0.2889  loss_rpn_cls: 0.0192  loss_rpn_loc: 0.02274  time: 0.5827  data_time: 0.0129  lr: 0.001  max_mem: 6307M
[32m[03/25 07:06:44 d2.utils.events]: [39m eta: 2:09:31  iter: 1619  total_loss: 0.7883  loss_cls: 0.4298  loss_box_reg: 0.3151  loss_rpn_cls: 0.02846  loss_rpn_loc: 0.02294  time: 0.5826  data_time: 0.0127  lr: 0.001  max_mem: 6307M
[32m[03/25 07:06:56 d2.utils.events]: [39m eta: 2:09:19  iter: 1639  total_loss: 0.7881  loss_cls: 0.4084  loss_box_reg: 0.2784  loss_rpn_cls: 0.02254  loss_rpn_loc: 0.0214  time: 0.5826  data_time: 0.0126  lr: 0.001  max_mem: 6307M
[32m[03/25 07:07:08 d2.utils.events]: [39m eta: 2:09:07  iter: 1659  total_loss: 0.7657  loss_cls: 0.4578  loss_box_reg: 0.268  loss_rpn_cls: 0.02557  loss_rpn_loc: 0.02816  time: 0.5826  data_time: 0.0136  lr: 0.001  max_mem: 6307M
[32m[03/25 07:07:19 d2.utils.events]: [39m eta: 2:08:56  iter: 1679  total_loss: 0.7435  loss_cls: 0.3886  loss_box_reg: 0.2953  loss_rpn_cls: 0.01994  loss_rpn_loc: 0.0197  time: 0.5826  data_time: 0.0138  lr: 0.001  max_mem: 6307M
[32m[03/25 07:07:31 d2.utils.events]: [39m eta: 2:08:43  iter: 1699  total_loss: 0.7812  loss_cls: 0.4166  loss_box_reg: 0.2895  loss_rpn_cls: 0.03225  loss_rpn_loc: 0.03568  time: 0.5826  data_time: 0.0133  lr: 0.001  max_mem: 6307M
[32m[03/25 07:07:43 d2.utils.events]: [39m eta: 2:08:31  iter: 1719  total_loss: 0.7397  loss_cls: 0.3843  loss_box_reg: 0.2961  loss_rpn_cls: 0.0278  loss_rpn_loc: 0.02625  time: 0.5826  data_time: 0.0134  lr: 0.001  max_mem: 6307M
[32m[03/25 07:07:54 d2.utils.events]: [39m eta: 2:08:19  iter: 1739  total_loss: 0.7481  loss_cls: 0.3806  loss_box_reg: 0.3311  loss_rpn_cls: 0.02253  loss_rpn_loc: 0.0255  time: 0.5826  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:08:06 d2.utils.events]: [39m eta: 2:08:08  iter: 1759  total_loss: 0.8469  loss_cls: 0.5005  loss_box_reg: 0.2924  loss_rpn_cls: 0.03548  loss_rpn_loc: 0.02473  time: 0.5826  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:08:18 d2.utils.events]: [39m eta: 2:07:56  iter: 1779  total_loss: 0.7165  loss_cls: 0.417  loss_box_reg: 0.2576  loss_rpn_cls: 0.01685  loss_rpn_loc: 0.02142  time: 0.5826  data_time: 0.0127  lr: 0.001  max_mem: 6307M
[32m[03/25 07:08:29 d2.utils.events]: [39m eta: 2:07:44  iter: 1799  total_loss: 0.7006  loss_cls: 0.3972  loss_box_reg: 0.2973  loss_rpn_cls: 0.02145  loss_rpn_loc: 0.0274  time: 0.5826  data_time: 0.0128  lr: 0.001  max_mem: 6307M
[32m[03/25 07:08:41 d2.utils.events]: [39m eta: 2:07:33  iter: 1819  total_loss: 0.8334  loss_cls: 0.4684  loss_box_reg: 0.2849  loss_rpn_cls: 0.03528  loss_rpn_loc: 0.02684  time: 0.5826  data_time: 0.0134  lr: 0.001  max_mem: 6307M
[32m[03/25 07:08:53 d2.utils.events]: [39m eta: 2:07:21  iter: 1839  total_loss: 0.668  loss_cls: 0.3928  loss_box_reg: 0.2373  loss_rpn_cls: 0.02025  loss_rpn_loc: 0.01863  time: 0.5825  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:09:04 d2.utils.events]: [39m eta: 2:07:10  iter: 1859  total_loss: 0.7239  loss_cls: 0.4302  loss_box_reg: 0.277  loss_rpn_cls: 0.02134  loss_rpn_loc: 0.02205  time: 0.5825  data_time: 0.0133  lr: 0.001  max_mem: 6307M
[32m[03/25 07:09:16 d2.utils.events]: [39m eta: 2:06:58  iter: 1879  total_loss: 0.7441  loss_cls: 0.3845  loss_box_reg: 0.2457  loss_rpn_cls: 0.01842  loss_rpn_loc: 0.01858  time: 0.5825  data_time: 0.0133  lr: 0.001  max_mem: 6307M
[32m[03/25 07:09:28 d2.utils.events]: [39m eta: 2:06:47  iter: 1899  total_loss: 0.8133  loss_cls: 0.4369  loss_box_reg: 0.3334  loss_rpn_cls: 0.01843  loss_rpn_loc: 0.0173  time: 0.5825  data_time: 0.0135  lr: 0.001  max_mem: 6307M
[32m[03/25 07:09:39 d2.utils.events]: [39m eta: 2:06:36  iter: 1919  total_loss: 0.8368  loss_cls: 0.428  loss_box_reg: 0.3593  loss_rpn_cls: 0.01662  loss_rpn_loc: 0.02047  time: 0.5825  data_time: 0.0133  lr: 0.001  max_mem: 6307M
[32m[03/25 07:09:51 d2.utils.events]: [39m eta: 2:06:24  iter: 1939  total_loss: 0.7618  loss_cls: 0.43  loss_box_reg: 0.2485  loss_rpn_cls: 0.01415  loss_rpn_loc: 0.03217  time: 0.5826  data_time: 0.0154  lr: 0.001  max_mem: 6307M
[32m[03/25 07:10:03 d2.utils.events]: [39m eta: 2:06:14  iter: 1959  total_loss: 0.8443  loss_cls: 0.4218  loss_box_reg: 0.3304  loss_rpn_cls: 0.02306  loss_rpn_loc: 0.01735  time: 0.5826  data_time: 0.0139  lr: 0.001  max_mem: 6307M
[32m[03/25 07:10:14 d2.utils.events]: [39m eta: 2:06:03  iter: 1979  total_loss: 0.8179  loss_cls: 0.4349  loss_box_reg: 0.2851  loss_rpn_cls: 0.02005  loss_rpn_loc: 0.02587  time: 0.5826  data_time: 0.0137  lr: 0.001  max_mem: 6307M
[32m[03/25 07:10:26 d2.utils.events]: [39m eta: 2:05:51  iter: 1999  total_loss: 0.7209  loss_cls: 0.3922  loss_box_reg: 0.2922  loss_rpn_cls: 0.01564  loss_rpn_loc: 0.0249  time: 0.5826  data_time: 0.0131  lr: 0.001  max_mem: 6307M
[32m[03/25 07:10:38 d2.utils.events]: [39m eta: 2:05:40  iter: 2019  total_loss: 0.7196  loss_cls: 0.4278  loss_box_reg: 0.2445  loss_rpn_cls: 0.01732  loss_rpn_loc: 0.01794  time: 0.5826  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:10:49 d2.utils.events]: [39m eta: 2:05:28  iter: 2039  total_loss: 0.7683  loss_cls: 0.4098  loss_box_reg: 0.2895  loss_rpn_cls: 0.02432  loss_rpn_loc: 0.02366  time: 0.5826  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:11:01 d2.utils.events]: [39m eta: 2:05:18  iter: 2059  total_loss: 0.8085  loss_cls: 0.4161  loss_box_reg: 0.341  loss_rpn_cls: 0.02039  loss_rpn_loc: 0.02426  time: 0.5826  data_time: 0.0131  lr: 0.001  max_mem: 6307M
[32m[03/25 07:11:13 d2.utils.events]: [39m eta: 2:05:07  iter: 2079  total_loss: 0.7813  loss_cls: 0.4063  loss_box_reg: 0.3164  loss_rpn_cls: 0.02132  loss_rpn_loc: 0.0256  time: 0.5826  data_time: 0.0142  lr: 0.001  max_mem: 6307M
[32m[03/25 07:11:24 d2.utils.events]: [39m eta: 2:04:57  iter: 2099  total_loss: 0.788  loss_cls: 0.4434  loss_box_reg: 0.3166  loss_rpn_cls: 0.02056  loss_rpn_loc: 0.02243  time: 0.5826  data_time: 0.0140  lr: 0.001  max_mem: 6307M
[32m[03/25 07:11:36 d2.utils.events]: [39m eta: 2:04:45  iter: 2119  total_loss: 0.732  loss_cls: 0.4002  loss_box_reg: 0.2477  loss_rpn_cls: 0.01463  loss_rpn_loc: 0.02485  time: 0.5826  data_time: 0.0142  lr: 0.001  max_mem: 6307M
[32m[03/25 07:11:48 d2.utils.events]: [39m eta: 2:04:33  iter: 2139  total_loss: 0.7775  loss_cls: 0.3808  loss_box_reg: 0.3146  loss_rpn_cls: 0.02197  loss_rpn_loc: 0.03181  time: 0.5826  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:11:59 d2.utils.events]: [39m eta: 2:04:21  iter: 2159  total_loss: 0.7446  loss_cls: 0.3975  loss_box_reg: 0.2507  loss_rpn_cls: 0.02219  loss_rpn_loc: 0.02728  time: 0.5826  data_time: 0.0144  lr: 0.001  max_mem: 6307M
[32m[03/25 07:12:11 d2.utils.events]: [39m eta: 2:04:09  iter: 2179  total_loss: 0.7851  loss_cls: 0.4144  loss_box_reg: 0.2943  loss_rpn_cls: 0.01649  loss_rpn_loc: 0.03338  time: 0.5826  data_time: 0.0135  lr: 0.001  max_mem: 6307M
[32m[03/25 07:12:23 d2.utils.events]: [39m eta: 2:03:58  iter: 2199  total_loss: 0.545  loss_cls: 0.3398  loss_box_reg: 0.2111  loss_rpn_cls: 0.01186  loss_rpn_loc: 0.0126  time: 0.5826  data_time: 0.0135  lr: 0.001  max_mem: 6307M
[32m[03/25 07:12:34 d2.utils.events]: [39m eta: 2:03:46  iter: 2219  total_loss: 0.7826  loss_cls: 0.3929  loss_box_reg: 0.3223  loss_rpn_cls: 0.02948  loss_rpn_loc: 0.04151  time: 0.5826  data_time: 0.0132  lr: 0.001  max_mem: 6307M
[32m[03/25 07:12:46 d2.utils.events]: [39m eta: 2:03:34  iter: 2239  total_loss: 0.6605  loss_cls: 0.3811  loss_box_reg: 0.2421  loss_rpn_cls: 0.01381  loss_rpn_loc: 0.009122  time: 0.5826  data_time: 0.0150  lr: 0.001  max_mem: 6307M
[32m[03/25 07:12:58 d2.utils.events]: [39m eta: 2:03:23  iter: 2259  total_loss: 0.9001  loss_cls: 0.47  loss_box_reg: 0.3083  loss_rpn_cls: 0.03165  loss_rpn_loc: 0.03373  time: 0.5826  data_time: 0.0140  lr: 0.001  max_mem: 6307M
[32m[03/25 07:13:01 d2.engine.hooks]: [39mOverall training speed: 2264 iterations in 0:21:59 (0.5827 s / it)
[32m[03/25 07:13:01 d2.engine.hooks]: [39mTotal training time: 0:22:00 (0:00:01 on hooks)
[32m[03/25 07:13:01 d2.utils.events]: [39m eta: 2:03:18  iter: 2266  total_loss: 0.8402  loss_cls: 0.47  loss_box_reg: 0.2893  loss_rpn_cls: 0.02751  loss_rpn_loc: 0.03059  time: 0.5826  data_time: 0.0138  lr: 0.001  max_mem: 6307M
Traceback (most recent call last):
  File "train.py", line 116, in <module>
    trainer.train()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 491, in train
    super().train(self.start_iter, self.max_iter)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 501, in run_step
    self._trainer.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 276, in run_step
    loss_dict = self.model(data)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/meta_arch/rcnn.py", line 157, in forward
    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/proposal_generator/rpn.py", line 471, in forward
    gt_labels, gt_boxes = self.label_and_sample_anchors(anchors, gt_instances)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/proposal_generator/rpn.py", line 340, in label_and_sample_anchors
    matched_idxs, gt_labels_i = retry_if_cuda_oom(self.anchor_matcher)(match_quality_matrix)
  File "/opt/ml/detection/baseline/detectron2/detectron2/utils/memory.py", line 70, in wrapped
    return func(*args, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/matcher.py", line 88, in __call__
    assert torch.all(match_quality_matrix >= 0)
KeyboardInterrupt