Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.
Some model parameters or buffers are not found in the checkpoint:
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}
[34mroi_heads.box_predictor.cls_score.{bias, weight}
[32m[03/25 06:21:55 d2.engine.defaults]: [39mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=11, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)
    )
  )
)
[32m[03/25 06:21:55 d2.data.datasets.coco]: [39mLoaded 4883 images in COCO format from ../../dataset/train.json
[32m[03/25 06:21:55 d2.data.build]: [39mRemoved 0 images with no usable annotations. 4883 images left.
[32m[03/25 06:21:55 d2.data.build]: [39mDistribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
[36m|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
[36m| General trash | 3966         |    Paper    | 6352         | Paper pack | 897          |
[36m|     Metal     | 936          |    Glass    | 982          |  Plastic   | 2943         |
[36m|   Styrofoam   | 1263         | Plastic bag | 5178         |  Battery   | 159          |
[36m|   Clothing    | 468          |             |              |            |              |
[36m|     total     | 23144        |             |              |            |              |
[32m[03/25 06:21:55 d2.data.build]: [39mUsing training sampler TrainingSampler
[32m[03/25 06:21:55 d2.data.common]: [39mSerializing 4883 elements to byte tensors and concatenating them all ...
[32m[03/25 06:21:55 d2.data.common]: [39mSerialized dataset takes 2.19 MiB
expected_results:
[]
[32m[03/25 06:21:56 d2.engine.train_loop]: [39mStarting training from iteration 0
/opt/ml/detection/baseline/detectron2/detectron2/modeling/roi_heads/fast_rcnn.py:103: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  num_fg = fg_inds.nonzero().numel()
{'loss_cls': 2.4706175327301025, 'loss_box_reg': 0.6988027691841125, 'loss_rpn_cls': 0.03837255761027336, 'loss_rpn_loc': 0.01770966127514839}
{'loss_cls': 2.47049880027771, 'loss_box_reg': 0.6902251243591309, 'loss_rpn_cls': 0.28469616174697876, 'loss_rpn_loc': 0.08496083319187164}
{'loss_cls': 2.4574174880981445, 'loss_box_reg': 0.5543163418769836, 'loss_rpn_cls': 0.034974198788404465, 'loss_rpn_loc': 0.019279247149825096}
{'loss_cls': 2.4524130821228027, 'loss_box_reg': 0.7770967483520508, 'loss_rpn_cls': 0.07912351936101913, 'loss_rpn_loc': 0.03488951921463013}
{'loss_cls': 2.4834182262420654, 'loss_box_reg': 0.6866163611412048, 'loss_rpn_cls': 0.05853588506579399, 'loss_rpn_loc': 0.029694296419620514}
{'loss_cls': 2.4805572032928467, 'loss_box_reg': 0.741369366645813, 'loss_rpn_cls': 0.25382888317108154, 'loss_rpn_loc': 0.09222647547721863}
{'loss_cls': 2.4678232669830322, 'loss_box_reg': 0.848118782043457, 'loss_rpn_cls': 0.05261560156941414, 'loss_rpn_loc': 0.02623922750353813}
{'loss_cls': 2.4486756324768066, 'loss_box_reg': 0.7399709820747375, 'loss_rpn_cls': 0.2478257715702057, 'loss_rpn_loc': 0.057736750692129135}
{'loss_cls': 2.4309403896331787, 'loss_box_reg': 0.7702805995941162, 'loss_rpn_cls': 0.07121989130973816, 'loss_rpn_loc': 0.03150037303566933}
{'loss_cls': 2.4512100219726562, 'loss_box_reg': 0.6818641424179077, 'loss_rpn_cls': 0.404960960149765, 'loss_rpn_loc': 0.1040666401386261}
{'loss_cls': 2.432109832763672, 'loss_box_reg': 0.8777506351470947, 'loss_rpn_cls': 0.3094038665294647, 'loss_rpn_loc': 0.042428549379110336}
{'loss_cls': 2.4357638359069824, 'loss_box_reg': 0.7473235130310059, 'loss_rpn_cls': 0.12627677619457245, 'loss_rpn_loc': 0.0301175769418478}
{'loss_cls': 2.4301037788391113, 'loss_box_reg': 0.6391066312789917, 'loss_rpn_cls': 0.26139354705810547, 'loss_rpn_loc': 0.10517263412475586}
{'loss_cls': 2.439668893814087, 'loss_box_reg': 0.6493169069290161, 'loss_rpn_cls': 0.01677737943828106, 'loss_rpn_loc': 0.006437315605580807}
{'loss_cls': 2.3690366744995117, 'loss_box_reg': 0.5796126127243042, 'loss_rpn_cls': 0.08414378762245178, 'loss_rpn_loc': 0.04905528202652931}
{'loss_cls': 2.3833465576171875, 'loss_box_reg': 0.7481200695037842, 'loss_rpn_cls': 0.09035465866327286, 'loss_rpn_loc': 0.08000445365905762}
{'loss_cls': 2.406768560409546, 'loss_box_reg': 0.4637952744960785, 'loss_rpn_cls': 0.010814995504915714, 'loss_rpn_loc': 0.0033345031552016735}
{'loss_cls': 2.350085735321045, 'loss_box_reg': 0.6847872734069824, 'loss_rpn_cls': 0.2836363911628723, 'loss_rpn_loc': 0.043265875428915024}
{'loss_cls': 2.3381266593933105, 'loss_box_reg': 0.40211910009384155, 'loss_rpn_cls': 0.19146855175495148, 'loss_rpn_loc': 0.13289503753185272}
{'loss_cls': 2.3130505084991455, 'loss_box_reg': 0.5009050965309143, 'loss_rpn_cls': 0.039426274597644806, 'loss_rpn_loc': 0.02221769466996193}
[32m[03/25 06:22:08 d2.utils.events]: [39m eta: 2:25:00  iter: 19  total_loss: 1.648e+09  loss_cls: 2.438  loss_box_reg: 0.6884  loss_rpn_cls: 0.08725  loss_rpn_loc: 0.03866  time: 0.5817  data_time: 0.0271  lr: 1.9981e-05  max_mem: 6307M
{'loss_cls': 2.3047873973846436, 'loss_box_reg': 0.7819194793701172, 'loss_rpn_cls': 0.007281824480742216, 'loss_rpn_loc': 0.01586664840579033}
{'loss_cls': 2.2088828086853027, 'loss_box_reg': 0.7438627481460571, 'loss_rpn_cls': 0.19125083088874817, 'loss_rpn_loc': 0.0448543056845665}
{'loss_cls': 2.2003610134124756, 'loss_box_reg': 0.6800128221511841, 'loss_rpn_cls': 0.0013273166259750724, 'loss_rpn_loc': 0.00494503416121006}
{'loss_cls': 2.2152719497680664, 'loss_box_reg': 0.6007441878318787, 'loss_rpn_cls': 0.08955641090869904, 'loss_rpn_loc': 0.10519344359636307}
{'loss_cls': 2.1919846534729004, 'loss_box_reg': 0.6642378568649292, 'loss_rpn_cls': 0.33173322677612305, 'loss_rpn_loc': 0.1383562684059143}
{'loss_cls': 2.1785385608673096, 'loss_box_reg': 0.6384809017181396, 'loss_rpn_cls': 0.11400561034679413, 'loss_rpn_loc': 0.05459105968475342}
{'loss_cls': 2.150132894515991, 'loss_box_reg': 0.895222544670105, 'loss_rpn_cls': 0.07342745363712311, 'loss_rpn_loc': 0.03413476049900055}
{'loss_cls': 2.0696308612823486, 'loss_box_reg': 0.5990158915519714, 'loss_rpn_cls': 0.04213429242372513, 'loss_rpn_loc': 0.025732431560754776}
{'loss_cls': 2.085344076156616, 'loss_box_reg': 0.5763121843338013, 'loss_rpn_cls': 0.10405266284942627, 'loss_rpn_loc': 0.014881154522299767}
{'loss_cls': 2.023441791534424, 'loss_box_reg': 0.5722026824951172, 'loss_rpn_cls': 0.01343111414462328, 'loss_rpn_loc': 0.014827778562903404}
{'loss_cls': 2.0042026042938232, 'loss_box_reg': 0.6536053419113159, 'loss_rpn_cls': 0.17060235142707825, 'loss_rpn_loc': 0.11848913133144379}
{'loss_cls': 2.039686441421509, 'loss_box_reg': 0.8263424634933472, 'loss_rpn_cls': 0.018565453588962555, 'loss_rpn_loc': 0.06619999557733536}
{'loss_cls': 1.9443862438201904, 'loss_box_reg': 0.6032837629318237, 'loss_rpn_cls': 0.04554591700434685, 'loss_rpn_loc': 0.019985314458608627}
{'loss_cls': 2.031707286834717, 'loss_box_reg': 0.8645700812339783, 'loss_rpn_cls': 0.25990787148475647, 'loss_rpn_loc': 0.08747409284114838}
{'loss_cls': 1.8130261898040771, 'loss_box_reg': 0.6395243406295776, 'loss_rpn_cls': 0.19453269243240356, 'loss_rpn_loc': 0.05279004946351051}
{'loss_cls': 1.7916635274887085, 'loss_box_reg': 0.6354615688323975, 'loss_rpn_cls': 0.07372869551181793, 'loss_rpn_loc': 0.02855539508163929}
{'loss_cls': 1.6907304525375366, 'loss_box_reg': 0.5319850444793701, 'loss_rpn_cls': 0.0157616063952446, 'loss_rpn_loc': 0.0051562488079071045}
{'loss_cls': 1.6498034000396729, 'loss_box_reg': 0.4816395044326782, 'loss_rpn_cls': 0.003399486653506756, 'loss_rpn_loc': 0.013118889182806015}
{'loss_cls': 1.658899188041687, 'loss_box_reg': 0.6872931718826294, 'loss_rpn_cls': 0.01744881458580494, 'loss_rpn_loc': 0.005740546155720949}
{'loss_cls': 1.6896859407424927, 'loss_box_reg': 0.8188199996948242, 'loss_rpn_cls': 0.039642274379730225, 'loss_rpn_loc': 0.1676599532365799}
[32m[03/25 06:22:20 d2.utils.events]: [39m eta: 2:24:42  iter: 39  total_loss: 1.648e+09  loss_cls: 2.036  loss_box_reg: 0.6466  loss_rpn_cls: 0.05949  loss_rpn_loc: 0.03135  time: 0.5806  data_time: 0.0116  lr: 3.9961e-05  max_mem: 6307M
{'loss_cls': 1.6323221921920776, 'loss_box_reg': 0.7668768167495728, 'loss_rpn_cls': 0.16848379373550415, 'loss_rpn_loc': 0.06196743994951248}
{'loss_cls': 1.5583192110061646, 'loss_box_reg': 0.5735586881637573, 'loss_rpn_cls': 0.06368010491132736, 'loss_rpn_loc': 0.018648821860551834}
{'loss_cls': 1.6337559223175049, 'loss_box_reg': 0.6957373023033142, 'loss_rpn_cls': 0.06644141674041748, 'loss_rpn_loc': 0.17622673511505127}
{'loss_cls': 1.5818103551864624, 'loss_box_reg': 0.6867808699607849, 'loss_rpn_cls': 0.1495985984802246, 'loss_rpn_loc': 0.03646253049373627}
{'loss_cls': 1.5737106800079346, 'loss_box_reg': 0.8360048532485962, 'loss_rpn_cls': 0.3403274118900299, 'loss_rpn_loc': 0.08584216237068176}
{'loss_cls': 1.3893787860870361, 'loss_box_reg': 0.7420843839645386, 'loss_rpn_cls': 0.04305082559585571, 'loss_rpn_loc': 0.04368484392762184}
{'loss_cls': 1.3930732011795044, 'loss_box_reg': 0.6841684579849243, 'loss_rpn_cls': 0.20866666734218597, 'loss_rpn_loc': 0.04862263798713684}
{'loss_cls': 1.286268711090088, 'loss_box_reg': 0.4857058525085449, 'loss_rpn_cls': 0.01063684094697237, 'loss_rpn_loc': 0.01248222030699253}
{'loss_cls': 1.2339001893997192, 'loss_box_reg': 0.47751474380493164, 'loss_rpn_cls': 0.0062346430495381355, 'loss_rpn_loc': 0.0027933348901569843}
{'loss_cls': 1.1452821493148804, 'loss_box_reg': 0.5160953402519226, 'loss_rpn_cls': 0.008274074643850327, 'loss_rpn_loc': 0.013467700220644474}
{'loss_cls': 1.2905538082122803, 'loss_box_reg': 0.79860919713974, 'loss_rpn_cls': 0.06682419776916504, 'loss_rpn_loc': 0.08281245827674866}
{'loss_cls': 1.1956807374954224, 'loss_box_reg': 0.6660265922546387, 'loss_rpn_cls': 0.01853981427848339, 'loss_rpn_loc': 0.024213653057813644}
{'loss_cls': 1.1694047451019287, 'loss_box_reg': 0.643653154373169, 'loss_rpn_cls': 0.002279209205880761, 'loss_rpn_loc': 0.005760313011705875}
{'loss_cls': 1.2194180488586426, 'loss_box_reg': 0.8343467712402344, 'loss_rpn_cls': 0.02310132421553135, 'loss_rpn_loc': 0.009969592094421387}
{'loss_cls': 1.0186744928359985, 'loss_box_reg': 0.7236016392707825, 'loss_rpn_cls': 0.2955220639705658, 'loss_rpn_loc': 0.05577575042843819}
{'loss_cls': 0.9260454177856445, 'loss_box_reg': 0.5300198197364807, 'loss_rpn_cls': 0.014867102727293968, 'loss_rpn_loc': 0.004716064315289259}
{'loss_cls': 1.1641037464141846, 'loss_box_reg': 0.8889999389648438, 'loss_rpn_cls': 0.11757512390613556, 'loss_rpn_loc': 0.036352358758449554}
{'loss_cls': 1.0428194999694824, 'loss_box_reg': 0.7472999691963196, 'loss_rpn_cls': 0.03240937739610672, 'loss_rpn_loc': 0.017076482996344566}
{'loss_cls': 0.9168603420257568, 'loss_box_reg': 0.6107308864593506, 'loss_rpn_cls': 0.11013800650835037, 'loss_rpn_loc': 0.02145756036043167}
{'loss_cls': 0.950485110282898, 'loss_box_reg': 0.8185976147651672, 'loss_rpn_cls': 0.04350021481513977, 'loss_rpn_loc': 0.015559565275907516}
[32m[03/25 06:22:31 d2.utils.events]: [39m eta: 2:24:18  iter: 59  total_loss: 1.648e+09  loss_cls: 1.227  loss_box_reg: 0.6913  loss_rpn_cls: 0.05359  loss_rpn_loc: 0.02284  time: 0.5797  data_time: 0.0119  lr: 5.9941e-05  max_mem: 6307M
{'loss_cls': 0.9293184876441956, 'loss_box_reg': 0.6526361107826233, 'loss_rpn_cls': 0.10526316612958908, 'loss_rpn_loc': 0.0403951033949852}
{'loss_cls': 0.9207963943481445, 'loss_box_reg': 0.6452833414077759, 'loss_rpn_cls': 0.33872485160827637, 'loss_rpn_loc': 0.07232991605997086}
{'loss_cls': 0.7754179835319519, 'loss_box_reg': 0.6529674530029297, 'loss_rpn_cls': 0.009748978540301323, 'loss_rpn_loc': 0.005066049285233021}
{'loss_cls': 0.7408196926116943, 'loss_box_reg': 0.565808117389679, 'loss_rpn_cls': 0.0391688197851181, 'loss_rpn_loc': 0.0077872793190181255}
{'loss_cls': 0.6900730729103088, 'loss_box_reg': 0.49582791328430176, 'loss_rpn_cls': 0.009399250149726868, 'loss_rpn_loc': 0.007313531823456287}
{'loss_cls': 0.982460081577301, 'loss_box_reg': 0.930812418460846, 'loss_rpn_cls': 0.10647203028202057, 'loss_rpn_loc': 0.09377895295619965}
{'loss_cls': 0.8357040882110596, 'loss_box_reg': 0.7074184417724609, 'loss_rpn_cls': 0.16088345646858215, 'loss_rpn_loc': 0.0552244558930397}
{'loss_cls': 0.6685274243354797, 'loss_box_reg': 0.5676785707473755, 'loss_rpn_cls': 0.00706609757617116, 'loss_rpn_loc': 0.009493021294474602}
{'loss_cls': 0.8266863822937012, 'loss_box_reg': 0.8079854249954224, 'loss_rpn_cls': 0.010448810644447803, 'loss_rpn_loc': 0.024559935554862022}
{'loss_cls': 0.7646749019622803, 'loss_box_reg': 0.7145191431045532, 'loss_rpn_cls': 0.06175413727760315, 'loss_rpn_loc': 0.027879483997821808}
{'loss_cls': 0.7851084470748901, 'loss_box_reg': 0.7688727378845215, 'loss_rpn_cls': 0.035097841173410416, 'loss_rpn_loc': 0.027371399104595184}
{'loss_cls': 0.85338294506073, 'loss_box_reg': 0.7237552404403687, 'loss_rpn_cls': 0.04028571397066116, 'loss_rpn_loc': 0.019998125731945038}
{'loss_cls': 0.6507442593574524, 'loss_box_reg': 0.6055338382720947, 'loss_rpn_cls': 0.01503196731209755, 'loss_rpn_loc': 0.009919777512550354}
{'loss_cls': 0.7397064566612244, 'loss_box_reg': 0.6168363094329834, 'loss_rpn_cls': 0.024658311158418655, 'loss_rpn_loc': 0.011140073649585247}
{'loss_cls': 0.8059893250465393, 'loss_box_reg': 0.6619240045547485, 'loss_rpn_cls': 0.09803300350904465, 'loss_rpn_loc': 0.033199843019247055}
{'loss_cls': 0.7136617302894592, 'loss_box_reg': 0.6090629696846008, 'loss_rpn_cls': 0.03259621560573578, 'loss_rpn_loc': 0.027081239968538284}
{'loss_cls': 0.7378384470939636, 'loss_box_reg': 0.6813591718673706, 'loss_rpn_cls': 0.11983705312013626, 'loss_rpn_loc': 0.05540383607149124}
{'loss_cls': 0.9852443337440491, 'loss_box_reg': 0.862903356552124, 'loss_rpn_cls': 0.11667981743812561, 'loss_rpn_loc': 0.0753401443362236}
{'loss_cls': 0.6526830792427063, 'loss_box_reg': 0.6218180656433105, 'loss_rpn_cls': 0.00845243688672781, 'loss_rpn_loc': 0.010084322653710842}
{'loss_cls': 0.7483432292938232, 'loss_box_reg': 0.7198219895362854, 'loss_rpn_cls': 0.038503535091876984, 'loss_rpn_loc': 0.1082608550786972}
[32m[03/25 06:22:43 d2.utils.events]: [39m eta: 2:24:08  iter: 79  total_loss: 1.648e+09  loss_cls: 0.77  loss_box_reg: 0.6574  loss_rpn_cls: 0.03884  loss_rpn_loc: 0.02723  time: 0.5804  data_time: 0.0118  lr: 7.9921e-05  max_mem: 6307M
{'loss_cls': 0.8014150261878967, 'loss_box_reg': 0.6825182437896729, 'loss_rpn_cls': 0.378274142742157, 'loss_rpn_loc': 0.053625453263521194}
{'loss_cls': 0.6651618480682373, 'loss_box_reg': 0.6380179524421692, 'loss_rpn_cls': 0.002426579361781478, 'loss_rpn_loc': 0.0045252349227666855}
{'loss_cls': 0.7268033027648926, 'loss_box_reg': 0.6774545907974243, 'loss_rpn_cls': 0.06235891953110695, 'loss_rpn_loc': 0.08169159293174744}
{'loss_cls': 1.0259418487548828, 'loss_box_reg': 0.9301273822784424, 'loss_rpn_cls': 0.33324745297431946, 'loss_rpn_loc': 0.07896170020103455}
{'loss_cls': 0.662960946559906, 'loss_box_reg': 0.6468490958213806, 'loss_rpn_cls': 0.0382404662668705, 'loss_rpn_loc': 0.01346019096672535}
{'loss_cls': 0.8211588263511658, 'loss_box_reg': 0.6843246817588806, 'loss_rpn_cls': 0.13776372373104095, 'loss_rpn_loc': 0.11790947616100311}
{'loss_cls': 0.9686988592147827, 'loss_box_reg': 0.933455228805542, 'loss_rpn_cls': 0.10685957223176956, 'loss_rpn_loc': 0.05947771668434143}
{'loss_cls': 0.9373401999473572, 'loss_box_reg': 0.9758216738700867, 'loss_rpn_cls': 0.08644521236419678, 'loss_rpn_loc': 0.06322531402111053}
{'loss_cls': 0.753691554069519, 'loss_box_reg': 0.7024511694908142, 'loss_rpn_cls': 0.04624151065945625, 'loss_rpn_loc': 0.014937030151486397}
{'loss_cls': 0.8128219246864319, 'loss_box_reg': 0.8334009647369385, 'loss_rpn_cls': 0.43911057710647583, 'loss_rpn_loc': 0.07275620847940445}
{'loss_cls': 0.7029638290405273, 'loss_box_reg': 0.6933027505874634, 'loss_rpn_cls': 0.5977319478988647, 'loss_rpn_loc': 0.10410748422145844}
{'loss_cls': 0.715997040271759, 'loss_box_reg': 0.7262341976165771, 'loss_rpn_cls': 0.1824786216020584, 'loss_rpn_loc': 0.024406693875789642}
{'loss_cls': 0.6085337996482849, 'loss_box_reg': 0.5656906366348267, 'loss_rpn_cls': 0.001738542108796537, 'loss_rpn_loc': 0.006495871115475893}
{'loss_cls': 0.72612065076828, 'loss_box_reg': 0.6910985112190247, 'loss_rpn_cls': 0.018725693225860596, 'loss_rpn_loc': 0.009034703485667706}
{'loss_cls': 0.6635117530822754, 'loss_box_reg': 0.6564534306526184, 'loss_rpn_cls': 0.019466694444417953, 'loss_rpn_loc': 0.00974654033780098}
{'loss_cls': 0.6624563932418823, 'loss_box_reg': 0.6849285364151001, 'loss_rpn_cls': 0.02674814872443676, 'loss_rpn_loc': 0.005907595623284578}
{'loss_cls': 0.6312261819839478, 'loss_box_reg': 0.5840170979499817, 'loss_rpn_cls': 0.0778004452586174, 'loss_rpn_loc': 0.027347438037395477}
{'loss_cls': 0.8101063966751099, 'loss_box_reg': 0.7178554534912109, 'loss_rpn_cls': 0.0961686447262764, 'loss_rpn_loc': 0.03041672147810459}
{'loss_cls': 0.8231747150421143, 'loss_box_reg': 0.776686429977417, 'loss_rpn_cls': 0.12872377038002014, 'loss_rpn_loc': 0.04649084433913231}
{'loss_cls': 0.846095860004425, 'loss_box_reg': 0.8447484970092773, 'loss_rpn_cls': 0.035372961312532425, 'loss_rpn_loc': 0.0320977084338665}
[32m[03/25 06:22:55 d2.utils.events]: [39m eta: 2:24:04  iter: 99  total_loss: 1.648e+09  loss_cls: 0.7402  loss_box_reg: 0.6922  loss_rpn_cls: 0.08212  loss_rpn_loc: 0.03126  time: 0.5808  data_time: 0.0118  lr: 9.9901e-05  max_mem: 6307M
{'loss_cls': 0.8532953858375549, 'loss_box_reg': 0.7616471648216248, 'loss_rpn_cls': 0.6537816524505615, 'loss_rpn_loc': 0.13643619418144226}
{'loss_cls': 0.6886678338050842, 'loss_box_reg': 0.6809099912643433, 'loss_rpn_cls': 0.032216284424066544, 'loss_rpn_loc': 0.016738921403884888}
{'loss_cls': 0.6809034943580627, 'loss_box_reg': 0.6933228373527527, 'loss_rpn_cls': 0.02464366890490055, 'loss_rpn_loc': 0.014635298401117325}
{'loss_cls': 0.989124059677124, 'loss_box_reg': 0.9596575498580933, 'loss_rpn_cls': 0.027424585074186325, 'loss_rpn_loc': 0.030525842681527138}
{'loss_cls': 0.9267198443412781, 'loss_box_reg': 0.9327930212020874, 'loss_rpn_cls': 0.0502854585647583, 'loss_rpn_loc': 0.02187493070960045}
{'loss_cls': 0.7106596827507019, 'loss_box_reg': 0.6104145050048828, 'loss_rpn_cls': 0.041550733149051666, 'loss_rpn_loc': 0.0802529975771904}
{'loss_cls': 0.7544916868209839, 'loss_box_reg': 0.7491998672485352, 'loss_rpn_cls': 0.1658402979373932, 'loss_rpn_loc': 0.03737570717930794}
{'loss_cls': 0.4769843518733978, 'loss_box_reg': 0.48174089193344116, 'loss_rpn_cls': 0.005684056784957647, 'loss_rpn_loc': 0.0028651705943048}
{'loss_cls': 0.9412484169006348, 'loss_box_reg': 0.7400826215744019, 'loss_rpn_cls': 0.33425575494766235, 'loss_rpn_loc': 0.11019566655158997}
{'loss_cls': 0.6721963882446289, 'loss_box_reg': 0.6825366616249084, 'loss_rpn_cls': 0.03161964192986488, 'loss_rpn_loc': 0.02420639619231224}
{'loss_cls': 0.8091456294059753, 'loss_box_reg': 0.7296954393386841, 'loss_rpn_cls': 0.022877788171172142, 'loss_rpn_loc': 0.009603044018149376}
{'loss_cls': 0.9438393115997314, 'loss_box_reg': 1.0060913562774658, 'loss_rpn_cls': 0.0461449958384037, 'loss_rpn_loc': 0.05677332729101181}
{'loss_cls': 0.7644742727279663, 'loss_box_reg': 0.7877812385559082, 'loss_rpn_cls': 0.05674498528242111, 'loss_rpn_loc': 0.028265248984098434}
{'loss_cls': 0.7214877009391785, 'loss_box_reg': 0.616012454032898, 'loss_rpn_cls': 0.03362208232283592, 'loss_rpn_loc': 0.006227786652743816}
{'loss_cls': 0.7914325594902039, 'loss_box_reg': 0.7378751039505005, 'loss_rpn_cls': 0.08543367683887482, 'loss_rpn_loc': 0.049987033009529114}
{'loss_cls': 0.6162962913513184, 'loss_box_reg': 0.6035380363464355, 'loss_rpn_cls': 0.010589191690087318, 'loss_rpn_loc': 0.00659719854593277}
{'loss_cls': 0.9844541549682617, 'loss_box_reg': 0.9499475955963135, 'loss_rpn_cls': 0.042815834283828735, 'loss_rpn_loc': 0.08066925406455994}
{'loss_cls': 0.6289698481559753, 'loss_box_reg': 0.6291135549545288, 'loss_rpn_cls': 0.008896809071302414, 'loss_rpn_loc': 0.11782844364643097}
{'loss_cls': 0.6776936650276184, 'loss_box_reg': 0.6643261313438416, 'loss_rpn_cls': 0.09771937131881714, 'loss_rpn_loc': 0.022921089082956314}
{'loss_cls': 0.7377550601959229, 'loss_box_reg': 0.7535296678543091, 'loss_rpn_cls': 0.02367345243692398, 'loss_rpn_loc': 0.013556456193327904}
[32m[03/25 06:23:06 d2.utils.events]: [39m eta: 2:24:01  iter: 119  total_loss: 1.648e+09  loss_cls: 0.7461  loss_box_reg: 0.7338  loss_rpn_cls: 0.03759  loss_rpn_loc: 0.02624  time: 0.5810  data_time: 0.0118  lr: 0.00011988  max_mem: 6307M
{'loss_cls': 0.9497398734092712, 'loss_box_reg': 0.8779945969581604, 'loss_rpn_cls': 0.5213429927825928, 'loss_rpn_loc': 0.16319262981414795}
{'loss_cls': 0.8948025703430176, 'loss_box_reg': 0.8389112949371338, 'loss_rpn_cls': 0.11798464506864548, 'loss_rpn_loc': 0.06426215916872025}
[32m[03/25 06:23:08 d2.engine.hooks]: [39mOverall training speed: 120 iterations in 0:01:09 (0.5815 s / it)
[32m[03/25 06:23:08 d2.engine.hooks]: [39mTotal training time: 0:01:09 (0:00:00 on hooks)
[32m[03/25 06:23:08 d2.utils.events]: [39m eta: 2:24:00  iter: 122  total_loss: 1.648e+09  loss_cls: 0.7595  loss_box_reg: 0.739  loss_rpn_cls: 0.04218  loss_rpn_loc: 0.0294  time: 0.5810  data_time: 0.0118  lr: 0.00012188  max_mem: 6307M
Traceback (most recent call last):
  File "train.py", line 116, in <module>
    trainer.train()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 491, in train
    super().train(self.start_iter, self.max_iter)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 501, in run_step
    self._trainer.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 276, in run_step
    loss_dict = self.model(data)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/meta_arch/rcnn.py", line 154, in forward
    features = self.backbone(images.tensor)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/backbone/fpn.py", line 126, in forward
    bottom_up_features = self.bottom_up(x)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/backbone/resnet.py", line 449, in forward
    x = stage(x)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/backbone/resnet.py", line 198, in forward
    out = self.conv2(out)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/layers/wrappers.py", line 88, in forward
    x = self.norm(x)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/layers/batch_norm.py", line 49, in forward
    bias = self.bias - self.running_mean * scale
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 765, in __getattr__
    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
KeyboardInterrupt