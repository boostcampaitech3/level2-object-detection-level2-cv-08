[32m[03/26 17:19:35 d2.engine.defaults]: [39mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.
Some model parameters or buffers are not found in the checkpoint:
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}
[34mroi_heads.box_predictor.cls_score.{bias, weight}
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=11, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)
    )
  )
)
[32m[03/26 17:19:36 d2.data.datasets.coco]: [39mLoaded 4883 images in COCO format from ../../dataset/train.json
[32m[03/26 17:19:36 d2.data.build]: [39mRemoved 0 images with no usable annotations. 4883 images left.
[32m[03/26 17:19:36 d2.data.build]: [39mDistribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
[36m|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
[36m| General trash | 3966         |    Paper    | 6352         | Paper pack | 897          |
[36m|     Metal     | 936          |    Glass    | 982          |  Plastic   | 2943         |
[36m|   Styrofoam   | 1263         | Plastic bag | 5178         |  Battery   | 159          |
[36m|   Clothing    | 468          |             |              |            |              |
[36m|     total     | 23144        |             |              |            |              |
[32m[03/26 17:19:36 d2.data.build]: [39mUsing training sampler RepeatFactorTrainingSampler
[32m[03/26 17:19:36 d2.data.common]: [39mSerializing 4883 elements to byte tensors and concatenating them all ...
[32m[03/26 17:19:36 d2.data.common]: [39mSerialized dataset takes 2.19 MiB
expected_results:
[]
[32m[03/26 17:19:36 d2.engine.train_loop]: [39mStarting training from iteration 0
/opt/ml/detection/baseline/detectron2/detectron2/modeling/roi_heads/fast_rcnn.py:103: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  num_fg = fg_inds.nonzero().numel()
/opt/conda/envs/detection/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[32m[03/26 17:19:38 d2.utils.events]: [39m eta: 1:34:45  iter: 4  total_loss: 3.534  loss_cls: 2.407  loss_box_reg: 0.3985  loss_rpn_cls: 0.6923  loss_rpn_loc: 0.03131  time: 0.1958  data_time: 0.0806  lr: 4.996e-06  max_mem: 1512M
[32m[03/26 17:19:39 d2.utils.events]: [39m eta: 1:31:50  iter: 9  total_loss: 3.673  loss_cls: 2.395  loss_box_reg: 0.4175  loss_rpn_cls: 0.7166  loss_rpn_loc: 0.03794  time: 0.1806  data_time: 0.0442  lr: 9.991e-06  max_mem: 1512M
[32m[03/26 17:19:40 d2.utils.events]: [39m eta: 1:31:49  iter: 14  total_loss: 3.75  loss_cls: 2.38  loss_box_reg: 0.3985  loss_rpn_cls: 0.7408  loss_rpn_loc: 0.04457  time: 0.1770  data_time: 0.0324  lr: 1.4986e-05  max_mem: 1513M
[32m[03/26 17:19:41 d2.utils.events]: [39m eta: 1:31:43  iter: 19  total_loss: 3.781  loss_cls: 2.335  loss_box_reg: 0.4426  loss_rpn_cls: 1.097  loss_rpn_loc: 0.06367  time: 0.1755  data_time: 0.0263  lr: 1.9981e-05  max_mem: 1513M
[32m[03/26 17:19:42 d2.utils.events]: [39m eta: 1:31:47  iter: 24  total_loss: 3.804  loss_cls: 2.158  loss_box_reg: 0.5303  loss_rpn_cls: 1.237  loss_rpn_loc: 0.07411  time: 0.1752  data_time: 0.0082  lr: 2.4976e-05  max_mem: 1513M
[32m[03/26 17:19:43 d2.utils.events]: [39m eta: 1:31:46  iter: 29  total_loss: 3.728  loss_cls: 1.891  loss_box_reg: 0.5933  loss_rpn_cls: 1.237  loss_rpn_loc: 0.07411  time: 0.1746  data_time: 0.0082  lr: 2.9971e-05  max_mem: 1513M
[32m[03/26 17:19:43 d2.utils.events]: [39m eta: 1:31:49  iter: 34  total_loss: 3.315  loss_cls: 1.64  loss_box_reg: 0.6414  loss_rpn_cls: 0.9553  loss_rpn_loc: 0.06814  time: 0.1749  data_time: 0.0081  lr: 3.4966e-05  max_mem: 1513M
[32m[03/26 17:19:44 d2.utils.events]: [39m eta: 1:31:53  iter: 39  total_loss: 2.612  loss_cls: 1.428  loss_box_reg: 0.6383  loss_rpn_cls: 0.4962  loss_rpn_loc: 0.02982  time: 0.1751  data_time: 0.0081  lr: 3.9961e-05  max_mem: 1513M
[32m[03/26 17:19:45 d2.utils.events]: [39m eta: 1:32:26  iter: 44  total_loss: 2.343  loss_cls: 1.185  loss_box_reg: 0.6727  loss_rpn_cls: 0.508  loss_rpn_loc: 0.04267  time: 0.1754  data_time: 0.0081  lr: 4.4956e-05  max_mem: 1513M
[32m[03/26 17:19:46 d2.utils.events]: [39m eta: 1:32:43  iter: 49  total_loss: 2.276  loss_cls: 1.107  loss_box_reg: 0.7435  loss_rpn_cls: 0.508  loss_rpn_loc: 0.04584  time: 0.1754  data_time: 0.0081  lr: 4.9951e-05  max_mem: 1513M
[32m[03/26 17:19:47 d2.utils.events]: [39m eta: 1:32:56  iter: 54  total_loss: 2.22  loss_cls: 0.9987  loss_box_reg: 0.7399  loss_rpn_cls: 0.4708  loss_rpn_loc: 0.05092  time: 0.1757  data_time: 0.0082  lr: 5.4946e-05  max_mem: 1513M
[32m[03/26 17:19:48 d2.utils.events]: [39m eta: 1:33:14  iter: 59  total_loss: 2.22  loss_cls: 0.9161  loss_box_reg: 0.7659  loss_rpn_cls: 0.4477  loss_rpn_loc: 0.05534  time: 0.1775  data_time: 0.0082  lr: 5.9941e-05  max_mem: 1513M
[32m[03/26 17:19:49 d2.utils.events]: [39m eta: 1:33:29  iter: 64  total_loss: 2.157  loss_cls: 0.869  loss_box_reg: 0.7529  loss_rpn_cls: 0.4247  loss_rpn_loc: 0.05187  time: 0.1775  data_time: 0.0081  lr: 6.4936e-05  max_mem: 1513M
[32m[03/26 17:19:50 d2.utils.events]: [39m eta: 1:33:30  iter: 69  total_loss: 1.943  loss_cls: 0.8377  loss_box_reg: 0.7401  loss_rpn_cls: 0.4001  loss_rpn_loc: 0.06302  time: 0.1774  data_time: 0.0081  lr: 6.9931e-05  max_mem: 1513M
[32m[03/26 17:19:51 d2.utils.events]: [39m eta: 1:33:34  iter: 74  total_loss: 1.867  loss_cls: 0.8145  loss_box_reg: 0.7476  loss_rpn_cls: 0.3075  loss_rpn_loc: 0.06401  time: 0.1774  data_time: 0.0080  lr: 7.4926e-05  max_mem: 1513M
[32m[03/26 17:19:52 d2.utils.events]: [39m eta: 1:33:34  iter: 79  total_loss: 1.949  loss_cls: 0.8274  loss_box_reg: 0.7371  loss_rpn_cls: 0.194  loss_rpn_loc: 0.06401  time: 0.1773  data_time: 0.0082  lr: 7.9921e-05  max_mem: 1513M
[32m[03/26 17:19:52 d2.utils.events]: [39m eta: 1:33:33  iter: 84  total_loss: 1.949  loss_cls: 0.8218  loss_box_reg: 0.7347  loss_rpn_cls: 0.1845  loss_rpn_loc: 0.06126  time: 0.1772  data_time: 0.0083  lr: 8.4916e-05  max_mem: 1515M
[32m[03/26 17:19:53 d2.utils.events]: [39m eta: 1:33:28  iter: 89  total_loss: 1.888  loss_cls: 0.8035  loss_box_reg: 0.7223  loss_rpn_cls: 0.1723  loss_rpn_loc: 0.05661  time: 0.1770  data_time: 0.0083  lr: 8.9911e-05  max_mem: 1515M
[32m[03/26 17:19:54 d2.utils.events]: [39m eta: 1:33:27  iter: 94  total_loss: 1.827  loss_cls: 0.7885  loss_box_reg: 0.6889  loss_rpn_cls: 0.1216  loss_rpn_loc: 0.0444  time: 0.1771  data_time: 0.0082  lr: 9.4906e-05  max_mem: 1515M
[32m[03/26 17:19:55 d2.utils.events]: [39m eta: 1:33:30  iter: 99  total_loss: 1.793  loss_cls: 0.7638  loss_box_reg: 0.7101  loss_rpn_cls: 0.1555  loss_rpn_loc: 0.05184  time: 0.1770  data_time: 0.0081  lr: 9.9901e-05  max_mem: 1516M
[32m[03/26 17:19:56 d2.utils.events]: [39m eta: 1:33:33  iter: 104  total_loss: 1.844  loss_cls: 0.7885  loss_box_reg: 0.7225  loss_rpn_cls: 0.1605  loss_rpn_loc: 0.04952  time: 0.1770  data_time: 0.0080  lr: 0.0001049  max_mem: 1516M
[32m[03/26 17:19:57 d2.utils.events]: [39m eta: 1:33:35  iter: 109  total_loss: 1.791  loss_cls: 0.7562  loss_box_reg: 0.719  loss_rpn_cls: 0.1475  loss_rpn_loc: 0.04633  time: 0.1770  data_time: 0.0079  lr: 0.00010989  max_mem: 1516M
[32m[03/26 17:19:58 d2.utils.events]: [39m eta: 1:33:38  iter: 114  total_loss: 1.817  loss_cls: 0.7991  loss_box_reg: 0.7428  loss_rpn_cls: 0.174  loss_rpn_loc: 0.05449  time: 0.1779  data_time: 0.0079  lr: 0.00011489  max_mem: 1516M
[32m[03/26 17:19:59 d2.utils.events]: [39m eta: 1:33:37  iter: 119  total_loss: 1.708  loss_cls: 0.728  loss_box_reg: 0.7324  loss_rpn_cls: 0.1509  loss_rpn_loc: 0.04976  time: 0.1781  data_time: 0.0077  lr: 0.00011988  max_mem: 1516M
[32m[03/26 17:20:00 d2.utils.events]: [39m eta: 1:33:42  iter: 124  total_loss: 1.708  loss_cls: 0.7463  loss_box_reg: 0.7501  loss_rpn_cls: 0.1411  loss_rpn_loc: 0.04007  time: 0.1782  data_time: 0.0078  lr: 0.00012488  max_mem: 1516M
[32m[03/26 17:20:01 d2.utils.events]: [39m eta: 1:33:41  iter: 129  total_loss: 1.71  loss_cls: 0.7723  loss_box_reg: 0.7667  loss_rpn_cls: 0.1045  loss_rpn_loc: 0.03026  time: 0.1781  data_time: 0.0077  lr: 0.00012987  max_mem: 1516M
[32m[03/26 17:20:02 d2.utils.events]: [39m eta: 1:33:39  iter: 134  total_loss: 1.712  loss_cls: 0.782  loss_box_reg: 0.7668  loss_rpn_cls: 0.09088  loss_rpn_loc: 0.01753  time: 0.1780  data_time: 0.0077  lr: 0.00013487  max_mem: 1516M
[32m[03/26 17:20:02 d2.utils.events]: [39m eta: 1:33:41  iter: 139  total_loss: 1.712  loss_cls: 0.7964  loss_box_reg: 0.7614  loss_rpn_cls: 0.1178  loss_rpn_loc: 0.03973  time: 0.1780  data_time: 0.0078  lr: 0.00013986  max_mem: 1516M
[32m[03/26 17:20:03 d2.utils.events]: [39m eta: 1:33:38  iter: 144  total_loss: 1.621  loss_cls: 0.7459  loss_box_reg: 0.7527  loss_rpn_cls: 0.1277  loss_rpn_loc: 0.04147  time: 0.1780  data_time: 0.0078  lr: 0.00014486  max_mem: 1518M
[32m[03/26 17:20:04 d2.utils.events]: [39m eta: 1:33:36  iter: 149  total_loss: 1.559  loss_cls: 0.7006  loss_box_reg: 0.7395  loss_rpn_cls: 0.1094  loss_rpn_loc: 0.03738  time: 0.1779  data_time: 0.0079  lr: 0.00014985  max_mem: 1518M
[32m[03/26 17:20:05 d2.utils.events]: [39m eta: 1:33:31  iter: 154  total_loss: 1.55  loss_cls: 0.6819  loss_box_reg: 0.7323  loss_rpn_cls: 0.09739  loss_rpn_loc: 0.03008  time: 0.1778  data_time: 0.0080  lr: 0.00015485  max_mem: 1518M
[32m[03/26 17:20:06 d2.utils.events]: [39m eta: 1:33:32  iter: 159  total_loss: 1.586  loss_cls: 0.6993  loss_box_reg: 0.7187  loss_rpn_cls: 0.09701  loss_rpn_loc: 0.03008  time: 0.1779  data_time: 0.0080  lr: 0.00015984  max_mem: 1518M
[32m[03/26 17:20:07 d2.utils.events]: [39m eta: 1:33:35  iter: 164  total_loss: 1.586  loss_cls: 0.6908  loss_box_reg: 0.6997  loss_rpn_cls: 0.08783  loss_rpn_loc: 0.02312  time: 0.1779  data_time: 0.0081  lr: 0.00016484  max_mem: 1518M
[32m[03/26 17:20:08 d2.utils.events]: [39m eta: 1:33:36  iter: 169  total_loss: 1.636  loss_cls: 0.7172  loss_box_reg: 0.7376  loss_rpn_cls: 0.1102  loss_rpn_loc: 0.03082  time: 0.1781  data_time: 0.0080  lr: 0.00016983  max_mem: 1518M
[32m[03/26 17:20:09 d2.utils.events]: [39m eta: 1:33:35  iter: 174  total_loss: 1.623  loss_cls: 0.713  loss_box_reg: 0.7316  loss_rpn_cls: 0.1102  loss_rpn_loc: 0.02781  time: 0.1780  data_time: 0.0079  lr: 0.00017483  max_mem: 1518M
[32m[03/26 17:20:10 d2.utils.events]: [39m eta: 1:33:32  iter: 179  total_loss: 1.624  loss_cls: 0.7057  loss_box_reg: 0.7537  loss_rpn_cls: 0.1148  loss_rpn_loc: 0.02781  time: 0.1779  data_time: 0.0079  lr: 0.00017982  max_mem: 1518M
[32m[03/26 17:20:11 d2.utils.events]: [39m eta: 1:33:26  iter: 184  total_loss: 1.66  loss_cls: 0.7382  loss_box_reg: 0.7583  loss_rpn_cls: 0.12  loss_rpn_loc: 0.03679  time: 0.1777  data_time: 0.0078  lr: 0.00018482  max_mem: 1518M
[32m[03/26 17:20:11 d2.utils.events]: [39m eta: 1:33:21  iter: 189  total_loss: 1.636  loss_cls: 0.718  loss_box_reg: 0.7537  loss_rpn_cls: 0.1111  loss_rpn_loc: 0.03266  time: 0.1775  data_time: 0.0078  lr: 0.00018981  max_mem: 1518M
[32m[03/26 17:20:12 d2.utils.events]: [39m eta: 1:33:23  iter: 194  total_loss: 1.668  loss_cls: 0.7253  loss_box_reg: 0.7355  loss_rpn_cls: 0.1659  loss_rpn_loc: 0.05093  time: 0.1776  data_time: 0.0083  lr: 0.00019481  max_mem: 1518M
[32m[03/26 17:20:13 d2.utils.events]: [39m eta: 1:33:19  iter: 199  total_loss: 1.66  loss_cls: 0.718  loss_box_reg: 0.7355  loss_rpn_cls: 0.1688  loss_rpn_loc: 0.04676  time: 0.1775  data_time: 0.0084  lr: 0.0001998  max_mem: 1518M
[32m[03/26 17:20:14 d2.utils.events]: [39m eta: 1:33:22  iter: 204  total_loss: 1.696  loss_cls: 0.7208  loss_box_reg: 0.7393  loss_rpn_cls: 0.1929  loss_rpn_loc: 0.06626  time: 0.1777  data_time: 0.0091  lr: 0.0002048  max_mem: 1518M
[32m[03/26 17:20:15 d2.utils.events]: [39m eta: 1:33:20  iter: 209  total_loss: 1.657  loss_cls: 0.683  loss_box_reg: 0.7393  loss_rpn_cls: 0.1748  loss_rpn_loc: 0.04945  time: 0.1777  data_time: 0.0093  lr: 0.00020979  max_mem: 1518M
[32m[03/26 17:20:16 d2.utils.events]: [39m eta: 1:33:18  iter: 214  total_loss: 1.552  loss_cls: 0.6721  loss_box_reg: 0.7393  loss_rpn_cls: 0.1535  loss_rpn_loc: 0.03646  time: 0.1776  data_time: 0.0088  lr: 0.00021479  max_mem: 1518M
[32m[03/26 17:20:17 d2.utils.events]: [39m eta: 1:33:18  iter: 219  total_loss: 1.666  loss_cls: 0.7052  loss_box_reg: 0.7413  loss_rpn_cls: 0.129  loss_rpn_loc: 0.03513  time: 0.1776  data_time: 0.0088  lr: 0.00021978  max_mem: 1518M
[32m[03/26 17:20:18 d2.utils.events]: [39m eta: 1:33:18  iter: 224  total_loss: 1.557  loss_cls: 0.6988  loss_box_reg: 0.7168  loss_rpn_cls: 0.1013  loss_rpn_loc: 0.02684  time: 0.1776  data_time: 0.0082  lr: 0.00022478  max_mem: 1518M
[32m[03/26 17:20:19 d2.utils.events]: [39m eta: 1:33:13  iter: 229  total_loss: 1.578  loss_cls: 0.7107  loss_box_reg: 0.6845  loss_rpn_cls: 0.1011  loss_rpn_loc: 0.02532  time: 0.1775  data_time: 0.0082  lr: 0.00022977  max_mem: 1518M
[32m[03/26 17:20:19 d2.utils.events]: [39m eta: 1:33:10  iter: 234  total_loss: 1.639  loss_cls: 0.7241  loss_box_reg: 0.7168  loss_rpn_cls: 0.103  loss_rpn_loc: 0.02414  time: 0.1774  data_time: 0.0081  lr: 0.00023477  max_mem: 1518M
[32m[03/26 17:20:20 d2.utils.events]: [39m eta: 1:33:07  iter: 239  total_loss: 1.545  loss_cls: 0.7075  loss_box_reg: 0.6953  loss_rpn_cls: 0.08525  loss_rpn_loc: 0.02281  time: 0.1773  data_time: 0.0081  lr: 0.00023976  max_mem: 1518M
[32m[03/26 17:20:21 d2.utils.events]: [39m eta: 1:33:05  iter: 244  total_loss: 1.543  loss_cls: 0.6896  loss_box_reg: 0.7066  loss_rpn_cls: 0.07102  loss_rpn_loc: 0.01963  time: 0.1772  data_time: 0.0081  lr: 0.00024476  max_mem: 1518M
[32m[03/26 17:20:22 d2.utils.events]: [39m eta: 1:33:02  iter: 249  total_loss: 1.543  loss_cls: 0.6857  loss_box_reg: 0.7364  loss_rpn_cls: 0.05706  loss_rpn_loc: 0.01963  time: 0.1771  data_time: 0.0080  lr: 0.00024975  max_mem: 1518M
[32m[03/26 17:20:23 d2.utils.events]: [39m eta: 1:33:03  iter: 254  total_loss: 1.469  loss_cls: 0.6762  loss_box_reg: 0.7092  loss_rpn_cls: 0.04867  loss_rpn_loc: 0.0178  time: 0.1772  data_time: 0.0083  lr: 0.00025475  max_mem: 1518M
[32m[03/26 17:20:24 d2.utils.events]: [39m eta: 1:33:00  iter: 259  total_loss: 1.436  loss_cls: 0.6502  loss_box_reg: 0.6782  loss_rpn_cls: 0.08327  loss_rpn_loc: 0.02174  time: 0.1771  data_time: 0.0086  lr: 0.00025974  max_mem: 1518M
[32m[03/26 17:20:25 d2.utils.events]: [39m eta: 1:32:57  iter: 264  total_loss: 1.487  loss_cls: 0.6595  loss_box_reg: 0.6975  loss_rpn_cls: 0.0854  loss_rpn_loc: 0.02825  time: 0.1771  data_time: 0.0089  lr: 0.00026474  max_mem: 1518M
[32m[03/26 17:20:26 d2.utils.events]: [39m eta: 1:32:56  iter: 269  total_loss: 1.487  loss_cls: 0.6595  loss_box_reg: 0.6724  loss_rpn_cls: 0.07245  loss_rpn_loc: 0.02348  time: 0.1771  data_time: 0.0090  lr: 0.00026973  max_mem: 1518M
[32m[03/26 17:20:26 d2.utils.events]: [39m eta: 1:32:55  iter: 274  total_loss: 1.521  loss_cls: 0.6502  loss_box_reg: 0.7051  loss_rpn_cls: 0.0854  loss_rpn_loc: 0.02622  time: 0.1770  data_time: 0.0090  lr: 0.00027473  max_mem: 1518M
[32m[03/26 17:20:27 d2.utils.events]: [39m eta: 1:32:54  iter: 279  total_loss: 1.559  loss_cls: 0.6645  loss_box_reg: 0.7465  loss_rpn_cls: 0.09289  loss_rpn_loc: 0.03009  time: 0.1770  data_time: 0.0092  lr: 0.00027972  max_mem: 1518M
[32m[03/26 17:20:28 d2.utils.events]: [39m eta: 1:32:53  iter: 284  total_loss: 1.616  loss_cls: 0.726  loss_box_reg: 0.759  loss_rpn_cls: 0.09654  loss_rpn_loc: 0.03314  time: 0.1770  data_time: 0.0097  lr: 0.00028472  max_mem: 1518M
[32m[03/26 17:20:29 d2.utils.events]: [39m eta: 1:32:51  iter: 289  total_loss: 1.616  loss_cls: 0.7137  loss_box_reg: 0.7538  loss_rpn_cls: 0.09654  loss_rpn_loc: 0.03314  time: 0.1769  data_time: 0.0097  lr: 0.00028971  max_mem: 1518M
[32m[03/26 17:20:30 d2.utils.events]: [39m eta: 1:32:52  iter: 294  total_loss: 1.575  loss_cls: 0.7001  loss_box_reg: 0.7371  loss_rpn_cls: 0.09123  loss_rpn_loc: 0.02985  time: 0.1770  data_time: 0.0104  lr: 0.00029471  max_mem: 1518M
[32m[03/26 17:20:31 d2.utils.events]: [39m eta: 1:32:52  iter: 299  total_loss: 1.597  loss_cls: 0.7238  loss_box_reg: 0.7363  loss_rpn_cls: 0.0857  loss_rpn_loc: 0.02642  time: 0.1771  data_time: 0.0101  lr: 0.0002997  max_mem: 1518M
[32m[03/26 17:20:32 d2.utils.events]: [39m eta: 1:32:54  iter: 304  total_loss: 1.574  loss_cls: 0.6895  loss_box_reg: 0.7122  loss_rpn_cls: 0.0857  loss_rpn_loc: 0.02642  time: 0.1771  data_time: 0.0094  lr: 0.0003047  max_mem: 1518M
[32m[03/26 17:20:33 d2.utils.events]: [39m eta: 1:32:52  iter: 309  total_loss: 1.643  loss_cls: 0.7212  loss_box_reg: 0.7073  loss_rpn_cls: 0.09965  loss_rpn_loc: 0.04556  time: 0.1770  data_time: 0.0093  lr: 0.00030969  max_mem: 1518M
[32m[03/26 17:20:34 d2.utils.events]: [39m eta: 1:32:49  iter: 314  total_loss: 1.508  loss_cls: 0.6491  loss_box_reg: 0.6927  loss_rpn_cls: 0.08556  loss_rpn_loc: 0.02913  time: 0.1769  data_time: 0.0082  lr: 0.00031469  max_mem: 1518M
[32m[03/26 17:20:35 d2.utils.events]: [39m eta: 1:32:47  iter: 319  total_loss: 1.515  loss_cls: 0.6598  loss_box_reg: 0.7046  loss_rpn_cls: 0.08821  loss_rpn_loc: 0.02535  time: 0.1769  data_time: 0.0080  lr: 0.00031968  max_mem: 1518M
[32m[03/26 17:20:35 d2.utils.events]: [39m eta: 1:32:44  iter: 324  total_loss: 1.433  loss_cls: 0.6357  loss_box_reg: 0.6825  loss_rpn_cls: 0.07394  loss_rpn_loc: 0.01273  time: 0.1768  data_time: 0.0078  lr: 0.00032468  max_mem: 1518M
[32m[03/26 17:20:36 d2.utils.events]: [39m eta: 1:32:43  iter: 329  total_loss: 1.46  loss_cls: 0.6584  loss_box_reg: 0.7243  loss_rpn_cls: 0.05629  loss_rpn_loc: 0.01659  time: 0.1767  data_time: 0.0078  lr: 0.00032967  max_mem: 1518M
[32m[03/26 17:20:37 d2.utils.events]: [39m eta: 1:32:40  iter: 334  total_loss: 1.599  loss_cls: 0.706  loss_box_reg: 0.7595  loss_rpn_cls: 0.1172  loss_rpn_loc: 0.02748  time: 0.1767  data_time: 0.0079  lr: 0.00033467  max_mem: 1518M
[32m[03/26 17:20:38 d2.utils.events]: [39m eta: 1:32:37  iter: 339  total_loss: 1.558  loss_cls: 0.6727  loss_box_reg: 0.7595  loss_rpn_cls: 0.09727  loss_rpn_loc: 0.02699  time: 0.1766  data_time: 0.0082  lr: 0.00033966  max_mem: 1518M
[32m[03/26 17:20:39 d2.utils.events]: [39m eta: 1:32:34  iter: 344  total_loss: 1.579  loss_cls: 0.6915  loss_box_reg: 0.7551  loss_rpn_cls: 0.07375  loss_rpn_loc: 0.02699  time: 0.1765  data_time: 0.0082  lr: 0.00034466  max_mem: 1518M
[32m[03/26 17:20:40 d2.utils.events]: [39m eta: 1:32:31  iter: 349  total_loss: 1.631  loss_cls: 0.7144  loss_box_reg: 0.7066  loss_rpn_cls: 0.08906  loss_rpn_loc: 0.02931  time: 0.1764  data_time: 0.0082  lr: 0.00034965  max_mem: 1518M
[32m[03/26 17:20:41 d2.utils.events]: [39m eta: 1:32:29  iter: 354  total_loss: 1.513  loss_cls: 0.6915  loss_box_reg: 0.6985  loss_rpn_cls: 0.07375  loss_rpn_loc: 0.03044  time: 0.1764  data_time: 0.0081  lr: 0.00035465  max_mem: 1518M
[32m[03/26 17:20:41 d2.utils.events]: [39m eta: 1:32:27  iter: 359  total_loss: 1.497  loss_cls: 0.6684  loss_box_reg: 0.6857  loss_rpn_cls: 0.09957  loss_rpn_loc: 0.03613  time: 0.1763  data_time: 0.0078  lr: 0.00035964  max_mem: 1518M
[32m[03/26 17:20:42 d2.utils.events]: [39m eta: 1:32:26  iter: 364  total_loss: 1.533  loss_cls: 0.6684  loss_box_reg: 0.6845  loss_rpn_cls: 0.1043  loss_rpn_loc: 0.05023  time: 0.1762  data_time: 0.0078  lr: 0.00036464  max_mem: 1518M
[32m[03/26 17:20:43 d2.utils.events]: [39m eta: 1:32:24  iter: 369  total_loss: 1.494  loss_cls: 0.6651  loss_box_reg: 0.6711  loss_rpn_cls: 0.09931  loss_rpn_loc: 0.04127  time: 0.1762  data_time: 0.0079  lr: 0.00036963  max_mem: 1518M
[32m[03/26 17:20:44 d2.utils.events]: [39m eta: 1:32:22  iter: 374  total_loss: 1.494  loss_cls: 0.6651  loss_box_reg: 0.6711  loss_rpn_cls: 0.09335  loss_rpn_loc: 0.02854  time: 0.1761  data_time: 0.0079  lr: 0.00037463  max_mem: 1518M
[32m[03/26 17:20:45 d2.utils.events]: [39m eta: 1:32:20  iter: 379  total_loss: 1.449  loss_cls: 0.6691  loss_box_reg: 0.6591  loss_rpn_cls: 0.08769  loss_rpn_loc: 0.02638  time: 0.1761  data_time: 0.0079  lr: 0.00037962  max_mem: 1519M
[32m[03/26 17:20:46 d2.utils.events]: [39m eta: 1:32:16  iter: 384  total_loss: 1.418  loss_cls: 0.6658  loss_box_reg: 0.6591  loss_rpn_cls: 0.07538  loss_rpn_loc: 0.02492  time: 0.1760  data_time: 0.0078  lr: 0.00038462  max_mem: 1519M
[32m[03/26 17:20:47 d2.utils.events]: [39m eta: 1:32:15  iter: 389  total_loss: 1.314  loss_cls: 0.6112  loss_box_reg: 0.6165  loss_rpn_cls: 0.05801  loss_rpn_loc: 0.0198  time: 0.1760  data_time: 0.0078  lr: 0.00038961  max_mem: 1519M
[32m[03/26 17:20:48 d2.utils.events]: [39m eta: 1:32:13  iter: 394  total_loss: 1.38  loss_cls: 0.6302  loss_box_reg: 0.6253  loss_rpn_cls: 0.06526  loss_rpn_loc: 0.02236  time: 0.1760  data_time: 0.0078  lr: 0.00039461  max_mem: 1519M
[32m[03/26 17:20:48 d2.utils.events]: [39m eta: 1:32:12  iter: 399  total_loss: 1.38  loss_cls: 0.6302  loss_box_reg: 0.6353  loss_rpn_cls: 0.06169  loss_rpn_loc: 0.02236  time: 0.1760  data_time: 0.0078  lr: 0.0003996  max_mem: 1519M
[32m[03/26 17:20:49 d2.utils.events]: [39m eta: 1:32:12  iter: 404  total_loss: 1.437  loss_cls: 0.6302  loss_box_reg: 0.6353  loss_rpn_cls: 0.06797  loss_rpn_loc: 0.032  time: 0.1759  data_time: 0.0079  lr: 0.0004046  max_mem: 1519M
[32m[03/26 17:20:50 d2.utils.events]: [39m eta: 1:32:09  iter: 409  total_loss: 1.437  loss_cls: 0.6416  loss_box_reg: 0.6341  loss_rpn_cls: 0.09874  loss_rpn_loc: 0.04513  time: 0.1759  data_time: 0.0079  lr: 0.00040959  max_mem: 1519M
[32m[03/26 17:20:51 d2.utils.events]: [39m eta: 1:32:06  iter: 414  total_loss: 1.454  loss_cls: 0.6394  loss_box_reg: 0.6257  loss_rpn_cls: 0.1251  loss_rpn_loc: 0.04523  time: 0.1759  data_time: 0.0080  lr: 0.00041459  max_mem: 1519M
[32m[03/26 17:20:52 d2.utils.events]: [39m eta: 1:32:05  iter: 419  total_loss: 1.421  loss_cls: 0.6648  loss_box_reg: 0.588  loss_rpn_cls: 0.1194  loss_rpn_loc: 0.04057  time: 0.1758  data_time: 0.0080  lr: 0.00041958  max_mem: 1519M
[32m[03/26 17:20:53 d2.utils.events]: [39m eta: 1:32:04  iter: 424  total_loss: 1.379  loss_cls: 0.6578  loss_box_reg: 0.5804  loss_rpn_cls: 0.08547  loss_rpn_loc: 0.0393  time: 0.1758  data_time: 0.0080  lr: 0.00042458  max_mem: 1519M
[32m[03/26 17:20:54 d2.utils.events]: [39m eta: 1:32:00  iter: 429  total_loss: 1.372  loss_cls: 0.6445  loss_box_reg: 0.5796  loss_rpn_cls: 0.0838  loss_rpn_loc: 0.03882  time: 0.1758  data_time: 0.0081  lr: 0.00042957  max_mem: 1519M
[32m[03/26 17:20:55 d2.utils.events]: [39m eta: 1:31:54  iter: 434  total_loss: 1.372  loss_cls: 0.6578  loss_box_reg: 0.5796  loss_rpn_cls: 0.0838  loss_rpn_loc: 0.03866  time: 0.1757  data_time: 0.0080  lr: 0.00043457  max_mem: 1519M
[32m[03/26 17:20:56 d2.utils.events]: [39m eta: 1:31:53  iter: 439  total_loss: 1.347  loss_cls: 0.6458  loss_box_reg: 0.5758  loss_rpn_cls: 0.08952  loss_rpn_loc: 0.04102  time: 0.1758  data_time: 0.0082  lr: 0.00043956  max_mem: 1519M
[32m[03/26 17:20:56 d2.utils.events]: [39m eta: 1:31:52  iter: 444  total_loss: 1.36  loss_cls: 0.6751  loss_box_reg: 0.5593  loss_rpn_cls: 0.09871  loss_rpn_loc: 0.04102  time: 0.1758  data_time: 0.0081  lr: 0.00044456  max_mem: 1519M
[32m[03/26 17:20:57 d2.utils.events]: [39m eta: 1:31:49  iter: 449  total_loss: 1.45  loss_cls: 0.7271  loss_box_reg: 0.5649  loss_rpn_cls: 0.1127  loss_rpn_loc: 0.04102  time: 0.1757  data_time: 0.0080  lr: 0.00044955  max_mem: 1519M
[32m[03/26 17:20:58 d2.utils.events]: [39m eta: 1:31:48  iter: 454  total_loss: 1.45  loss_cls: 0.7294  loss_box_reg: 0.5723  loss_rpn_cls: 0.1111  loss_rpn_loc: 0.0367  time: 0.1757  data_time: 0.0081  lr: 0.00045455  max_mem: 1519M
[32m[03/26 17:20:59 d2.utils.events]: [39m eta: 1:31:46  iter: 459  total_loss: 1.416  loss_cls: 0.7356  loss_box_reg: 0.58  loss_rpn_cls: 0.09442  loss_rpn_loc: 0.03199  time: 0.1757  data_time: 0.0078  lr: 0.00045954  max_mem: 1519M
[32m[03/26 17:21:00 d2.utils.events]: [39m eta: 1:31:44  iter: 464  total_loss: 1.416  loss_cls: 0.7237  loss_box_reg: 0.58  loss_rpn_cls: 0.111  loss_rpn_loc: 0.04253  time: 0.1756  data_time: 0.0078  lr: 0.00046454  max_mem: 1519M
[32m[03/26 17:21:01 d2.utils.events]: [39m eta: 1:31:43  iter: 469  total_loss: 1.433  loss_cls: 0.7209  loss_box_reg: 0.5538  loss_rpn_cls: 0.1085  loss_rpn_loc: 0.04612  time: 0.1756  data_time: 0.0078  lr: 0.00046953  max_mem: 1519M
[32m[03/26 17:21:02 d2.utils.events]: [39m eta: 1:31:39  iter: 474  total_loss: 1.413  loss_cls: 0.6519  loss_box_reg: 0.5233  loss_rpn_cls: 0.09582  loss_rpn_loc: 0.03692  time: 0.1756  data_time: 0.0077  lr: 0.00047453  max_mem: 1519M
[32m[03/26 17:21:03 d2.utils.events]: [39m eta: 1:31:41  iter: 479  total_loss: 1.401  loss_cls: 0.6444  loss_box_reg: 0.4799  loss_rpn_cls: 0.1028  loss_rpn_loc: 0.03692  time: 0.1756  data_time: 0.0078  lr: 0.00047952  max_mem: 1519M
[32m[03/26 17:21:03 d2.utils.events]: [39m eta: 1:31:40  iter: 484  total_loss: 1.401  loss_cls: 0.6444  loss_box_reg: 0.5135  loss_rpn_cls: 0.104  loss_rpn_loc: 0.04242  time: 0.1756  data_time: 0.0078  lr: 0.00048452  max_mem: 1519M
[32m[03/26 17:21:04 d2.utils.events]: [39m eta: 1:31:37  iter: 489  total_loss: 1.292  loss_cls: 0.6161  loss_box_reg: 0.4832  loss_rpn_cls: 0.09593  loss_rpn_loc: 0.03325  time: 0.1755  data_time: 0.0078  lr: 0.00048951  max_mem: 1519M
[32m[03/26 17:21:05 d2.utils.events]: [39m eta: 1:31:34  iter: 494  total_loss: 1.315  loss_cls: 0.6326  loss_box_reg: 0.4992  loss_rpn_cls: 0.09714  loss_rpn_loc: 0.03173  time: 0.1755  data_time: 0.0078  lr: 0.00049451  max_mem: 1519M
[32m[03/26 17:21:06 d2.utils.events]: [39m eta: 1:31:33  iter: 499  total_loss: 1.261  loss_cls: 0.6146  loss_box_reg: 0.4992  loss_rpn_cls: 0.09553  loss_rpn_loc: 0.03054  time: 0.1755  data_time: 0.0078  lr: 0.0004995  max_mem: 1519M
[32m[03/26 17:21:07 d2.utils.events]: [39m eta: 1:31:32  iter: 504  total_loss: 1.186  loss_cls: 0.6146  loss_box_reg: 0.4752  loss_rpn_cls: 0.08083  loss_rpn_loc: 0.02719  time: 0.1755  data_time: 0.0078  lr: 0.0005045  max_mem: 1519M
[32m[03/26 17:21:08 d2.utils.events]: [39m eta: 1:31:31  iter: 509  total_loss: 1.248  loss_cls: 0.6257  loss_box_reg: 0.505  loss_rpn_cls: 0.08851  loss_rpn_loc: 0.02943  time: 0.1754  data_time: 0.0078  lr: 0.00050949  max_mem: 1519M
[32m[03/26 17:21:09 d2.utils.events]: [39m eta: 1:31:30  iter: 514  total_loss: 1.158  loss_cls: 0.595  loss_box_reg: 0.4794  loss_rpn_cls: 0.08083  loss_rpn_loc: 0.03138  time: 0.1754  data_time: 0.0078  lr: 0.00051449  max_mem: 1519M
[32m[03/26 17:21:10 d2.utils.events]: [39m eta: 1:31:29  iter: 519  total_loss: 1.264  loss_cls: 0.5997  loss_box_reg: 0.4741  loss_rpn_cls: 0.09492  loss_rpn_loc: 0.04353  time: 0.1754  data_time: 0.0078  lr: 0.00051948  max_mem: 1519M
[32m[03/26 17:21:10 d2.utils.events]: [39m eta: 1:31:28  iter: 524  total_loss: 1.265  loss_cls: 0.6116  loss_box_reg: 0.4741  loss_rpn_cls: 0.1134  loss_rpn_loc: 0.04938  time: 0.1754  data_time: 0.0077  lr: 0.00052448  max_mem: 1519M
[32m[03/26 17:21:11 d2.utils.events]: [39m eta: 1:31:23  iter: 529  total_loss: 1.265  loss_cls: 0.5994  loss_box_reg: 0.4361  loss_rpn_cls: 0.1134  loss_rpn_loc: 0.05243  time: 0.1753  data_time: 0.0077  lr: 0.00052947  max_mem: 1519M
[32m[03/26 17:21:12 d2.utils.events]: [39m eta: 1:31:22  iter: 534  total_loss: 1.247  loss_cls: 0.6025  loss_box_reg: 0.433  loss_rpn_cls: 0.1194  loss_rpn_loc: 0.04938  time: 0.1753  data_time: 0.0077  lr: 0.00053447  max_mem: 1519M
[32m[03/26 17:21:13 d2.utils.events]: [39m eta: 1:31:19  iter: 539  total_loss: 1.138  loss_cls: 0.5932  loss_box_reg: 0.433  loss_rpn_cls: 0.09154  loss_rpn_loc: 0.03231  time: 0.1753  data_time: 0.0077  lr: 0.00053946  max_mem: 1519M
[32m[03/26 17:21:14 d2.utils.events]: [39m eta: 1:31:18  iter: 544  total_loss: 1.15  loss_cls: 0.5969  loss_box_reg: 0.4531  loss_rpn_cls: 0.09154  loss_rpn_loc: 0.03713  time: 0.1753  data_time: 0.0077  lr: 0.00054446  max_mem: 1519M
[32m[03/26 17:21:15 d2.utils.events]: [39m eta: 1:31:17  iter: 549  total_loss: 1.271  loss_cls: 0.6122  loss_box_reg: 0.5227  loss_rpn_cls: 0.1004  loss_rpn_loc: 0.04722  time: 0.1753  data_time: 0.0077  lr: 0.00054945  max_mem: 1519M
[32m[03/26 17:21:16 d2.utils.events]: [39m eta: 1:31:15  iter: 554  total_loss: 1.173  loss_cls: 0.5991  loss_box_reg: 0.4988  loss_rpn_cls: 0.08725  loss_rpn_loc: 0.03745  time: 0.1753  data_time: 0.0077  lr: 0.00055445  max_mem: 1519M
[32m[03/26 17:21:17 d2.utils.events]: [39m eta: 1:31:14  iter: 559  total_loss: 1.348  loss_cls: 0.636  loss_box_reg: 0.562  loss_rpn_cls: 0.09513  loss_rpn_loc: 0.0494  time: 0.1752  data_time: 0.0077  lr: 0.00055944  max_mem: 1519M
[32m[03/26 17:21:17 d2.utils.events]: [39m eta: 1:31:13  iter: 564  total_loss: 1.33  loss_cls: 0.6553  loss_box_reg: 0.4837  loss_rpn_cls: 0.1009  loss_rpn_loc: 0.05123  time: 0.1752  data_time: 0.0076  lr: 0.00056444  max_mem: 1519M
[32m[03/26 17:21:18 d2.utils.events]: [39m eta: 1:31:12  iter: 569  total_loss: 1.293  loss_cls: 0.6273  loss_box_reg: 0.46  loss_rpn_cls: 0.09447  loss_rpn_loc: 0.03042  time: 0.1752  data_time: 0.0076  lr: 0.00056943  max_mem: 1519M
[32m[03/26 17:21:19 d2.utils.events]: [39m eta: 1:31:10  iter: 574  total_loss: 1.287  loss_cls: 0.6287  loss_box_reg: 0.4338  loss_rpn_cls: 0.09967  loss_rpn_loc: 0.03557  time: 0.1752  data_time: 0.0076  lr: 0.00057443  max_mem: 1519M
[32m[03/26 17:21:20 d2.utils.events]: [39m eta: 1:31:09  iter: 579  total_loss: 1.111  loss_cls: 0.5891  loss_box_reg: 0.3856  loss_rpn_cls: 0.09235  loss_rpn_loc: 0.02747  time: 0.1752  data_time: 0.0076  lr: 0.00057942  max_mem: 1519M
[32m[03/26 17:21:21 d2.utils.events]: [39m eta: 1:31:08  iter: 584  total_loss: 1.111  loss_cls: 0.5875  loss_box_reg: 0.4338  loss_rpn_cls: 0.06357  loss_rpn_loc: 0.02747  time: 0.1752  data_time: 0.0076  lr: 0.00058442  max_mem: 1519M
[32m[03/26 17:21:22 d2.utils.events]: [39m eta: 1:31:08  iter: 589  total_loss: 1.121  loss_cls: 0.5982  loss_box_reg: 0.4352  loss_rpn_cls: 0.07996  loss_rpn_loc: 0.0369  time: 0.1751  data_time: 0.0077  lr: 0.00058941  max_mem: 1526M
[32m[03/26 17:21:23 d2.utils.events]: [39m eta: 1:31:07  iter: 594  total_loss: 1.196  loss_cls: 0.6122  loss_box_reg: 0.4484  loss_rpn_cls: 0.09561  loss_rpn_loc: 0.03534  time: 0.1751  data_time: 0.0077  lr: 0.00059441  max_mem: 1526M
[32m[03/26 17:21:24 d2.utils.events]: [39m eta: 1:31:06  iter: 599  total_loss: 1.272  loss_cls: 0.6296  loss_box_reg: 0.4594  loss_rpn_cls: 0.1194  loss_rpn_loc: 0.04738  time: 0.1751  data_time: 0.0076  lr: 0.0005994  max_mem: 1526M
[32m[03/26 17:21:24 d2.utils.events]: [39m eta: 1:31:05  iter: 604  total_loss: 1.272  loss_cls: 0.6296  loss_box_reg: 0.4343  loss_rpn_cls: 0.1313  loss_rpn_loc: 0.04932  time: 0.1751  data_time: 0.0077  lr: 0.0006044  max_mem: 1526M
[32m[03/26 17:21:25 d2.utils.events]: [39m eta: 1:31:04  iter: 609  total_loss: 1.183  loss_cls: 0.6262  loss_box_reg: 0.4303  loss_rpn_cls: 0.1054  loss_rpn_loc: 0.04164  time: 0.1752  data_time: 0.0081  lr: 0.00060939  max_mem: 1526M
[32m[03/26 17:21:26 d2.utils.events]: [39m eta: 1:31:03  iter: 614  total_loss: 1.146  loss_cls: 0.59  loss_box_reg: 0.4152  loss_rpn_cls: 0.1054  loss_rpn_loc: 0.04887  time: 0.1752  data_time: 0.0081  lr: 0.00061439  max_mem: 1526M
[32m[03/26 17:21:27 d2.utils.events]: [39m eta: 1:31:02  iter: 619  total_loss: 1.027  loss_cls: 0.513  loss_box_reg: 0.4102  loss_rpn_cls: 0.06742  loss_rpn_loc: 0.02879  time: 0.1751  data_time: 0.0081  lr: 0.00061938  max_mem: 1526M
[32m[03/26 17:21:28 d2.utils.events]: [39m eta: 1:31:01  iter: 624  total_loss: 1.139  loss_cls: 0.5992  loss_box_reg: 0.4156  loss_rpn_cls: 0.09827  loss_rpn_loc: 0.03445  time: 0.1752  data_time: 0.0081  lr: 0.00062438  max_mem: 1526M
[32m[03/26 17:21:29 d2.utils.events]: [39m eta: 1:31:00  iter: 629  total_loss: 1.166  loss_cls: 0.6198  loss_box_reg: 0.4109  loss_rpn_cls: 0.1044  loss_rpn_loc: 0.03938  time: 0.1751  data_time: 0.0077  lr: 0.00062937  max_mem: 1526M
[32m[03/26 17:21:30 d2.utils.events]: [39m eta: 1:31:00  iter: 634  total_loss: 1.384  loss_cls: 0.6478  loss_box_reg: 0.4446  loss_rpn_cls: 0.1225  loss_rpn_loc: 0.04787  time: 0.1751  data_time: 0.0077  lr: 0.00063437  max_mem: 1526M
[32m[03/26 17:21:31 d2.utils.events]: [39m eta: 1:30:58  iter: 639  total_loss: 1.395  loss_cls: 0.6622  loss_box_reg: 0.4739  loss_rpn_cls: 0.1254  loss_rpn_loc: 0.04939  time: 0.1751  data_time: 0.0077  lr: 0.00063936  max_mem: 1526M
[32m[03/26 17:21:32 d2.utils.events]: [39m eta: 1:30:57  iter: 644  total_loss: 1.309  loss_cls: 0.6471  loss_box_reg: 0.4397  loss_rpn_cls: 0.1262  loss_rpn_loc: 0.05777  time: 0.1751  data_time: 0.0077  lr: 0.00064436  max_mem: 1526M
[32m[03/26 17:21:32 d2.utils.events]: [39m eta: 1:30:56  iter: 649  total_loss: 1.351  loss_cls: 0.6622  loss_box_reg: 0.4415  loss_rpn_cls: 0.1133  loss_rpn_loc: 0.04252  time: 0.1751  data_time: 0.0076  lr: 0.00064935  max_mem: 1526M
[32m[03/26 17:21:33 d2.utils.events]: [39m eta: 1:30:56  iter: 654  total_loss: 1.214  loss_cls: 0.6671  loss_box_reg: 0.422  loss_rpn_cls: 0.1017  loss_rpn_loc: 0.03488  time: 0.1753  data_time: 0.0077  lr: 0.00065435  max_mem: 1526M
[32m[03/26 17:21:34 d2.utils.events]: [39m eta: 1:30:56  iter: 659  total_loss: 1.281  loss_cls: 0.6622  loss_box_reg: 0.4306  loss_rpn_cls: 0.1017  loss_rpn_loc: 0.0344  time: 0.1754  data_time: 0.0079  lr: 0.00065934  max_mem: 1526M
[32m[03/26 17:21:35 d2.utils.events]: [39m eta: 1:30:56  iter: 664  total_loss: 1.333  loss_cls: 0.6703  loss_box_reg: 0.4205  loss_rpn_cls: 0.1037  loss_rpn_loc: 0.03499  time: 0.1756  data_time: 0.0080  lr: 0.00066434  max_mem: 1526M
[32m[03/26 17:21:36 d2.utils.events]: [39m eta: 1:30:56  iter: 669  total_loss: 1.298  loss_cls: 0.6622  loss_box_reg: 0.4734  loss_rpn_cls: 0.1037  loss_rpn_loc: 0.03652  time: 0.1757  data_time: 0.0083  lr: 0.00066933  max_mem: 1526M
[32m[03/26 17:21:37 d2.utils.events]: [39m eta: 1:30:55  iter: 674  total_loss: 1.332  loss_cls: 0.653  loss_box_reg: 0.4813  loss_rpn_cls: 0.123  loss_rpn_loc: 0.05444  time: 0.1757  data_time: 0.0083  lr: 0.00067433  max_mem: 1526M
[32m[03/26 17:21:38 d2.utils.events]: [39m eta: 1:30:55  iter: 679  total_loss: 1.303  loss_cls: 0.6348  loss_box_reg: 0.4861  loss_rpn_cls: 0.1134  loss_rpn_loc: 0.04555  time: 0.1757  data_time: 0.0085  lr: 0.00067932  max_mem: 1526M
[32m[03/26 17:21:39 d2.utils.events]: [39m eta: 1:30:53  iter: 684  total_loss: 1.255  loss_cls: 0.6284  loss_box_reg: 0.4906  loss_rpn_cls: 0.09807  loss_rpn_loc: 0.04244  time: 0.1757  data_time: 0.0084  lr: 0.00068432  max_mem: 1526M
[32m[03/26 17:21:40 d2.utils.events]: [39m eta: 1:30:51  iter: 689  total_loss: 1.232  loss_cls: 0.613  loss_box_reg: 0.4625  loss_rpn_cls: 0.1269  loss_rpn_loc: 0.05849  time: 0.1756  data_time: 0.0081  lr: 0.00068931  max_mem: 1526M
[32m[03/26 17:21:41 d2.utils.events]: [39m eta: 1:30:51  iter: 694  total_loss: 1.184  loss_cls: 0.5946  loss_box_reg: 0.3953  loss_rpn_cls: 0.1122  loss_rpn_loc: 0.04778  time: 0.1757  data_time: 0.0083  lr: 0.00069431  max_mem: 1526M
[32m[03/26 17:21:42 d2.utils.events]: [39m eta: 1:30:50  iter: 699  total_loss: 1.163  loss_cls: 0.5877  loss_box_reg: 0.4175  loss_rpn_cls: 0.1138  loss_rpn_loc: 0.04778  time: 0.1757  data_time: 0.0081  lr: 0.0006993  max_mem: 1526M
[32m[03/26 17:21:43 d2.utils.events]: [39m eta: 1:30:48  iter: 704  total_loss: 1.139  loss_cls: 0.5833  loss_box_reg: 0.4287  loss_rpn_cls: 0.1078  loss_rpn_loc: 0.04747  time: 0.1756  data_time: 0.0081  lr: 0.0007043  max_mem: 1526M
[32m[03/26 17:21:43 d2.utils.events]: [39m eta: 1:30:47  iter: 709  total_loss: 1.139  loss_cls: 0.5833  loss_box_reg: 0.4287  loss_rpn_cls: 0.1078  loss_rpn_loc: 0.03937  time: 0.1756  data_time: 0.0081  lr: 0.00070929  max_mem: 1526M
[32m[03/26 17:21:44 d2.utils.events]: [39m eta: 1:30:46  iter: 714  total_loss: 1.115  loss_cls: 0.5351  loss_box_reg: 0.4406  loss_rpn_cls: 0.1078  loss_rpn_loc: 0.03733  time: 0.1756  data_time: 0.0078  lr: 0.00071429  max_mem: 1526M
[32m[03/26 17:21:45 d2.utils.events]: [39m eta: 1:30:45  iter: 719  total_loss: 1.284  loss_cls: 0.6518  loss_box_reg: 0.4322  loss_rpn_cls: 0.1145  loss_rpn_loc: 0.04091  time: 0.1756  data_time: 0.0080  lr: 0.00071928  max_mem: 1526M
[32m[03/26 17:21:46 d2.utils.events]: [39m eta: 1:30:44  iter: 724  total_loss: 1.335  loss_cls: 0.645  loss_box_reg: 0.4764  loss_rpn_cls: 0.1252  loss_rpn_loc: 0.04363  time: 0.1756  data_time: 0.0081  lr: 0.00072428  max_mem: 1526M
[32m[03/26 17:21:47 d2.utils.events]: [39m eta: 1:30:43  iter: 729  total_loss: 1.248  loss_cls: 0.629  loss_box_reg: 0.438  loss_rpn_cls: 0.1273  loss_rpn_loc: 0.04717  time: 0.1756  data_time: 0.0081  lr: 0.00072927  max_mem: 1526M
[32m[03/26 17:21:48 d2.utils.events]: [39m eta: 1:30:42  iter: 734  total_loss: 1.3  loss_cls: 0.6491  loss_box_reg: 0.46  loss_rpn_cls: 0.137  loss_rpn_loc: 0.05324  time: 0.1755  data_time: 0.0080  lr: 0.00073427  max_mem: 1526M
[32m[03/26 17:21:49 d2.utils.events]: [39m eta: 1:30:41  iter: 739  total_loss: 1.202  loss_cls: 0.6081  loss_box_reg: 0.4311  loss_rpn_cls: 0.137  loss_rpn_loc: 0.05053  time: 0.1756  data_time: 0.0088  lr: 0.00073926  max_mem: 1526M
[32m[03/26 17:21:50 d2.engine.hooks]: [39mOverall training speed: 742 iterations in 0:02:10 (0.1757 s / it)
[32m[03/26 17:21:50 d2.engine.hooks]: [39mTotal training time: 0:02:11 (0:00:01 on hooks)
[32m[03/26 17:21:50 d2.utils.events]: [39m eta: 1:30:41  iter: 744  total_loss: 1.233  loss_cls: 0.6081  loss_box_reg: 0.4534  loss_rpn_cls: 0.1354  loss_rpn_loc: 0.0498  time: 0.1756  data_time: 0.0088  lr: 0.00074326  max_mem: 1526M
Traceback (most recent call last):
  File "train.py", line 129, in <module>
    trainer.train()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 491, in train
    super().train(self.start_iter, self.max_iter)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 501, in run_step
    self._trainer.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 400, in run_step
    loss_dict = self.model(data)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/meta_arch/rcnn.py", line 157, in forward
    proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/proposal_generator/rpn.py", line 478, in forward
    anchors, pred_objectness_logits, pred_anchor_deltas, images.image_sizes
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/proposal_generator/rpn.py", line 511, in predict_proposals
    self.training,
  File "/opt/ml/detection/baseline/detectron2/detectron2/modeling/proposal_generator/proposal_utils.py", line 116, in find_top_rpn_proposals
    keep = batched_nms(boxes.tensor, scores_per_img, lvl, nms_thresh)
  File "/opt/ml/detection/baseline/detectron2/detectron2/layers/nms.py", line 21, in batched_nms
    return box_ops.batched_nms(boxes.float(), scores, idxs, iou_threshold)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/jit/_trace.py", line 1100, in wrapper
    return fn(*args, **kwargs)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torchvision/ops/boxes.py", line 88, in batched_nms
    keep = nms(boxes_for_nms, scores, iou_threshold)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torchvision/ops/boxes.py", line 42, in nms
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
KeyboardInterrupt