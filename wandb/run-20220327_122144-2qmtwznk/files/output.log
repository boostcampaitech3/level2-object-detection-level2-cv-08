[32m[03/27 12:21:50 d2.engine.defaults]: [39mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=384, out_features=192, bias=False)
            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1536, out_features=768, bias=False)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=11, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)
    )
  )
)
[32m[03/27 12:21:50 d2.data.datasets.coco]: [39mLoaded 4280 images in COCO format from ../../dataset/SK_train_annotations.json
[32m[03/27 12:21:50 d2.data.build]: [39mRemoved 0 images with no usable annotations. 4280 images left.
Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.
Some model parameters or buffers are not found in the checkpoint:
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}
[34mroi_heads.box_predictor.cls_score.{bias, weight}
The checkpoint state_dict contains keys that are not used by the model:
  [35mpixel_mean
  [35mpixel_std
/opt/ml/detection/baseline/detectron2/detectron2/modeling/roi_heads/fast_rcnn.py:103: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  num_fg = fg_inds.nonzero().numel()
/opt/conda/envs/detection/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[32m[03/27 12:21:51 d2.data.build]: [39mDistribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
[36m|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
[36m| General trash | 3468         |    Paper    | 5511         | Paper pack | 800          |
[36m|     Metal     | 827          |    Glass    | 900          |  Plastic   | 2591         |
[36m|   Styrofoam   | 1140         | Plastic bag | 4563         |  Battery   | 148          |
[36m|   Clothing    | 416          |             |              |            |              |
[36m|     total     | 20364        |             |              |            |              |
[32m[03/27 12:21:51 d2.data.build]: [39mUsing training sampler RepeatFactorTrainingSampler
[32m[03/27 12:21:51 d2.data.common]: [39mSerializing 4280 elements to byte tensors and concatenating them all ...
[32m[03/27 12:21:51 d2.data.common]: [39mSerialized dataset takes 1.92 MiB
expected_results:
[]
[32m[03/27 12:21:51 d2.engine.train_loop]: [39mStarting training from iteration 0
[32m[03/27 12:21:53 d2.utils.events]: [39m eta: 2:21:08  iter: 4  total_loss: 5.278  loss_cls: 2.552  loss_box_reg: 0.05044  loss_rpn_cls: 2.386  loss_rpn_loc: 0.1978  time: 0.1960  data_time: 0.0594  lr: 4.996e-07  max_mem: 2704M
[32m[03/27 12:21:54 d2.utils.events]: [39m eta: 2:29:12  iter: 9  total_loss: 4.265  loss_cls: 2.535  loss_box_reg: 0.06339  loss_rpn_cls: 1.485  loss_rpn_loc: 0.1308  time: 0.2096  data_time: 0.0335  lr: 9.991e-07  max_mem: 3054M
[32m[03/27 12:21:55 d2.utils.events]: [39m eta: 2:36:49  iter: 14  total_loss: 4.028  loss_cls: 2.506  loss_box_reg: 0.09373  loss_rpn_cls: 1.188  loss_rpn_loc: 0.09113  time: 0.2138  data_time: 0.0248  lr: 1.4986e-06  max_mem: 3054M
[32m[03/27 12:21:56 d2.utils.events]: [39m eta: 2:37:03  iter: 19  total_loss: 4.039  loss_cls: 2.487  loss_box_reg: 0.09426  loss_rpn_cls: 1.264  loss_rpn_loc: 0.1  time: 0.2157  data_time: 0.0205  lr: 1.9981e-06  max_mem: 3054M
[32m[03/27 12:21:58 d2.utils.events]: [39m eta: 2:37:11  iter: 24  total_loss: 4.039  loss_cls: 2.39  loss_box_reg: 0.1215  loss_rpn_cls: 1.264  loss_rpn_loc: 0.1  time: 0.2168  data_time: 0.0075  lr: 2.4976e-06  max_mem: 3054M
[32m[03/27 12:21:59 d2.utils.events]: [39m eta: 2:37:00  iter: 29  total_loss: 3.987  loss_cls: 2.208  loss_box_reg: 0.1215  loss_rpn_cls: 1.264  loss_rpn_loc: 0.1  time: 0.2174  data_time: 0.0076  lr: 2.9971e-06  max_mem: 3054M
[32m[03/27 12:22:00 d2.utils.events]: [39m eta: 2:37:09  iter: 34  total_loss: 3.631  loss_cls: 2.042  loss_box_reg: 0.1178  loss_rpn_cls: 1.459  loss_rpn_loc: 0.1267  time: 0.2184  data_time: 0.0076  lr: 3.4966e-06  max_mem: 3054M
[32m[03/27 12:22:01 d2.utils.events]: [39m eta: 2:37:17  iter: 39  total_loss: 3.029  loss_cls: 1.721  loss_box_reg: 0.1392  loss_rpn_cls: 1.069  loss_rpn_loc: 0.1021  time: 0.2192  data_time: 0.0077  lr: 3.9961e-06  max_mem: 3054M
[32m[03/27 12:22:02 d2.utils.events]: [39m eta: 2:37:35  iter: 44  total_loss: 2.26  loss_cls: 1.44  loss_box_reg: 0.1356  loss_rpn_cls: 0.7494  loss_rpn_loc: 0.07411  time: 0.2203  data_time: 0.0082  lr: 4.4956e-06  max_mem: 3054M
[32m[03/27 12:22:03 d2.utils.events]: [39m eta: 2:37:40  iter: 49  total_loss: 2.4  loss_cls: 1.197  loss_box_reg: 0.1905  loss_rpn_cls: 1.017  loss_rpn_loc: 0.1132  time: 0.2223  data_time: 0.0087  lr: 4.9951e-06  max_mem: 3054M
[32m[03/27 12:22:04 d2.utils.events]: [39m eta: 2:37:49  iter: 54  total_loss: 2.326  loss_cls: 1.008  loss_box_reg: 0.2258  loss_rpn_cls: 0.993  loss_rpn_loc: 0.1099  time: 0.2227  data_time: 0.0091  lr: 5.4946e-06  max_mem: 3054M
[32m[03/27 12:22:06 d2.utils.events]: [39m eta: 2:38:08  iter: 59  total_loss: 2.211  loss_cls: 0.9098  loss_box_reg: 0.2721  loss_rpn_cls: 0.9279  loss_rpn_loc: 0.1049  time: 0.2234  data_time: 0.0101  lr: 5.9941e-06  max_mem: 3054M
[32m[03/27 12:22:07 d2.utils.events]: [39m eta: 2:38:02  iter: 64  total_loss: 1.789  loss_cls: 0.7551  loss_box_reg: 0.3336  loss_rpn_cls: 0.6284  loss_rpn_loc: 0.1019  time: 0.2233  data_time: 0.0096  lr: 6.4936e-06  max_mem: 3054M
[32m[03/27 12:22:08 d2.utils.events]: [39m eta: 2:37:53  iter: 69  total_loss: 1.326  loss_cls: 0.6525  loss_box_reg: 0.3473  loss_rpn_cls: 0.3027  loss_rpn_loc: 0.05953  time: 0.2232  data_time: 0.0091  lr: 6.9931e-06  max_mem: 3054M
[32m[03/27 12:22:09 d2.utils.events]: [39m eta: 2:37:37  iter: 74  total_loss: 1.273  loss_cls: 0.6459  loss_box_reg: 0.3696  loss_rpn_cls: 0.2014  loss_rpn_loc: 0.04073  time: 0.2230  data_time: 0.0086  lr: 7.4926e-06  max_mem: 3054M
[32m[03/27 12:22:10 d2.utils.events]: [39m eta: 2:37:34  iter: 79  total_loss: 1.491  loss_cls: 0.7457  loss_box_reg: 0.4585  loss_rpn_cls: 0.2219  loss_rpn_loc: 0.04508  time: 0.2229  data_time: 0.0076  lr: 7.9921e-06  max_mem: 3054M
[32m[03/27 12:22:11 d2.utils.events]: [39m eta: 2:37:34  iter: 84  total_loss: 1.547  loss_cls: 0.773  loss_box_reg: 0.4927  loss_rpn_cls: 0.2134  loss_rpn_loc: 0.04789  time: 0.2228  data_time: 0.0078  lr: 8.4916e-06  max_mem: 3054M
[32m[03/27 12:22:12 d2.utils.events]: [39m eta: 2:37:37  iter: 89  total_loss: 1.616  loss_cls: 0.8245  loss_box_reg: 0.5493  loss_rpn_cls: 0.2038  loss_rpn_loc: 0.04485  time: 0.2230  data_time: 0.0078  lr: 8.9911e-06  max_mem: 3054M
[32m[03/27 12:22:13 d2.utils.events]: [39m eta: 2:37:32  iter: 94  total_loss: 1.626  loss_cls: 0.7828  loss_box_reg: 0.5791  loss_rpn_cls: 0.2038  loss_rpn_loc: 0.04485  time: 0.2228  data_time: 0.0078  lr: 9.4906e-06  max_mem: 3054M
[32m[03/27 12:22:14 d2.utils.events]: [39m eta: 2:37:26  iter: 99  total_loss: 1.439  loss_cls: 0.7042  loss_box_reg: 0.5127  loss_rpn_cls: 0.182  loss_rpn_loc: 0.04485  time: 0.2227  data_time: 0.0078  lr: 9.9901e-06  max_mem: 3054M
[32m[03/27 12:22:16 d2.utils.events]: [39m eta: 2:37:21  iter: 104  total_loss: 1.525  loss_cls: 0.7587  loss_box_reg: 0.5709  loss_rpn_cls: 0.168  loss_rpn_loc: 0.04446  time: 0.2225  data_time: 0.0076  lr: 1.049e-05  max_mem: 3054M
[32m[03/27 12:22:17 d2.utils.events]: [39m eta: 2:37:23  iter: 109  total_loss: 1.648  loss_cls: 0.7773  loss_box_reg: 0.5643  loss_rpn_cls: 0.1883  loss_rpn_loc: 0.05123  time: 0.2228  data_time: 0.0086  lr: 1.0989e-05  max_mem: 3054M
[32m[03/27 12:22:18 d2.utils.events]: [39m eta: 2:37:28  iter: 114  total_loss: 1.862  loss_cls: 0.8601  loss_box_reg: 0.6108  loss_rpn_cls: 0.2131  loss_rpn_loc: 0.05875  time: 0.2230  data_time: 0.0087  lr: 1.1489e-05  max_mem: 3054M
[32m[03/27 12:22:19 d2.utils.events]: [39m eta: 2:37:35  iter: 119  total_loss: 1.803  loss_cls: 0.8389  loss_box_reg: 0.6505  loss_rpn_cls: 0.2088  loss_rpn_loc: 0.05539  time: 0.2234  data_time: 0.0088  lr: 1.1988e-05  max_mem: 3054M
[32m[03/27 12:22:20 d2.utils.events]: [39m eta: 2:37:35  iter: 124  total_loss: 1.815  loss_cls: 0.8381  loss_box_reg: 0.6557  loss_rpn_cls: 0.2307  loss_rpn_loc: 0.07072  time: 0.2240  data_time: 0.0097  lr: 1.2488e-05  max_mem: 3054M
[32m[03/27 12:22:21 d2.utils.events]: [39m eta: 2:37:46  iter: 129  total_loss: 1.778  loss_cls: 0.8249  loss_box_reg: 0.6623  loss_rpn_cls: 0.2159  loss_rpn_loc: 0.06924  time: 0.2241  data_time: 0.0091  lr: 1.2987e-05  max_mem: 3054M
[32m[03/27 12:22:23 d2.utils.events]: [39m eta: 2:37:46  iter: 134  total_loss: 1.699  loss_cls: 0.7906  loss_box_reg: 0.6306  loss_rpn_cls: 0.1991  loss_rpn_loc: 0.05892  time: 0.2241  data_time: 0.0095  lr: 1.3487e-05  max_mem: 3054M
[32m[03/27 12:22:24 d2.utils.events]: [39m eta: 2:37:32  iter: 139  total_loss: 1.701  loss_cls: 0.8049  loss_box_reg: 0.6372  loss_rpn_cls: 0.2026  loss_rpn_loc: 0.05892  time: 0.2239  data_time: 0.0094  lr: 1.3986e-05  max_mem: 3054M
[32m[03/27 12:22:25 d2.utils.events]: [39m eta: 2:37:31  iter: 144  total_loss: 1.656  loss_cls: 0.7511  loss_box_reg: 0.571  loss_rpn_cls: 0.2122  loss_rpn_loc: 0.05686  time: 0.2238  data_time: 0.0086  lr: 1.4486e-05  max_mem: 3054M
[32m[03/27 12:22:26 d2.utils.events]: [39m eta: 2:37:43  iter: 149  total_loss: 1.636  loss_cls: 0.7596  loss_box_reg: 0.6415  loss_rpn_cls: 0.1405  loss_rpn_loc: 0.04735  time: 0.2238  data_time: 0.0083  lr: 1.4985e-05  max_mem: 3054M
[32m[03/27 12:22:27 d2.utils.events]: [39m eta: 2:37:49  iter: 154  total_loss: 1.62  loss_cls: 0.7596  loss_box_reg: 0.6107  loss_rpn_cls: 0.2243  loss_rpn_loc: 0.06525  time: 0.2238  data_time: 0.0079  lr: 1.5485e-05  max_mem: 3054M
[32m[03/27 12:22:28 d2.utils.events]: [39m eta: 2:37:44  iter: 159  total_loss: 1.598  loss_cls: 0.7293  loss_box_reg: 0.5839  loss_rpn_cls: 0.1742  loss_rpn_loc: 0.05867  time: 0.2238  data_time: 0.0080  lr: 1.5984e-05  max_mem: 3054M
[32m[03/27 12:22:29 d2.utils.events]: [39m eta: 2:37:39  iter: 164  total_loss: 1.577  loss_cls: 0.7711  loss_box_reg: 0.603  loss_rpn_cls: 0.1279  loss_rpn_loc: 0.0396  time: 0.2238  data_time: 0.0080  lr: 1.6484e-05  max_mem: 3054M
[32m[03/27 12:22:30 d2.utils.events]: [39m eta: 2:37:38  iter: 169  total_loss: 1.577  loss_cls: 0.7498  loss_box_reg: 0.5745  loss_rpn_cls: 0.1442  loss_rpn_loc: 0.0446  time: 0.2237  data_time: 0.0078  lr: 1.6983e-05  max_mem: 3054M
[32m[03/27 12:22:32 d2.utils.events]: [39m eta: 2:37:35  iter: 174  total_loss: 1.503  loss_cls: 0.7498  loss_box_reg: 0.5617  loss_rpn_cls: 0.1388  loss_rpn_loc: 0.04134  time: 0.2235  data_time: 0.0078  lr: 1.7483e-05  max_mem: 3054M
[32m[03/27 12:22:33 d2.utils.events]: [39m eta: 2:37:23  iter: 179  total_loss: 1.503  loss_cls: 0.7498  loss_box_reg: 0.5855  loss_rpn_cls: 0.166  loss_rpn_loc: 0.0436  time: 0.2234  data_time: 0.0077  lr: 1.7982e-05  max_mem: 3054M
[32m[03/27 12:22:34 d2.utils.events]: [39m eta: 2:37:20  iter: 184  total_loss: 1.661  loss_cls: 0.7844  loss_box_reg: 0.6667  loss_rpn_cls: 0.1909  loss_rpn_loc: 0.04545  time: 0.2233  data_time: 0.0076  lr: 1.8482e-05  max_mem: 3054M
[32m[03/27 12:22:35 d2.utils.events]: [39m eta: 2:37:17  iter: 189  total_loss: 1.764  loss_cls: 0.8261  loss_box_reg: 0.6977  loss_rpn_cls: 0.1909  loss_rpn_loc: 0.04954  time: 0.2232  data_time: 0.0077  lr: 1.8981e-05  max_mem: 3054M
[32m[03/27 12:22:36 d2.utils.events]: [39m eta: 2:37:11  iter: 194  total_loss: 1.797  loss_cls: 0.8506  loss_box_reg: 0.6977  loss_rpn_cls: 0.1797  loss_rpn_loc: 0.05693  time: 0.2232  data_time: 0.0076  lr: 1.9481e-05  max_mem: 3054M
Traceback (most recent call last):
  File "train.py", line 149, in <module>
    trainer.train()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 491, in train
    super().train(self.start_iter, self.max_iter)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 501, in run_step
    self._trainer.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 408, in run_step
    self.grad_scaler.scale(losses).backward()
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/envs/detection/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
[32m[03/27 12:22:37 d2.utils.events]: [39m eta: 2:37:08  iter: 199  total_loss: 1.826  loss_cls: 0.8506  loss_box_reg: 0.6842  loss_rpn_cls: 0.2109  loss_rpn_loc: 0.07035  time: 0.2231  data_time: 0.0078  lr: 1.998e-05  max_mem: 3054M
[32m[03/27 12:22:38 d2.engine.hooks]: [39mOverall training speed: 200 iterations in 0:00:44 (0.2240 s / it)
[32m[03/27 12:22:38 d2.engine.hooks]: [39mTotal training time: 0:00:45 (0:00:00 on hooks)
[32m[03/27 12:22:38 d2.utils.events]: [39m eta: 2:37:07  iter: 202  total_loss: 1.81  loss_cls: 0.8313  loss_box_reg: 0.6641  loss_rpn_cls: 0.2346  loss_rpn_loc: 0.07364  time: 0.2231  data_time: 0.0078  lr: 2.018e-05  max_mem: 3054M