Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (11, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (11,) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (40, 1024) in the model! You might want to double check if this is expected.
Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (40,) in the model! You might want to double check if this is expected.
Some model parameters or buffers are not found in the checkpoint:
[34mroi_heads.box_predictor.bbox_pred.{bias, weight}
[34mroi_heads.box_predictor.cls_score.{bias, weight}
[32m[03/27 02:06:54 d2.engine.defaults]: [39mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (6): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (7): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (8): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (9): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (10): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (11): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (12): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (13): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (14): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (15): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (16): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (17): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (18): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (19): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (20): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (21): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (22): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (activation): ReLU()
      )
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc_relu1): ReLU()
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
      (fc_relu2): ReLU()
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=11, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)
    )
  )
)
[32m[03/27 02:06:54 d2.data.datasets.coco]: [39mLoaded 4280 images in COCO format from ../../dataset/SK_train_annotations.json
[32m[03/27 02:06:54 d2.data.build]: [39mRemoved 0 images with no usable annotations. 4280 images left.
[32m[03/27 02:06:54 d2.data.build]: [39mDistribution of instances among all 10 categories:
[36m|   category    | #instances   |  category   | #instances   |  category  | #instances   |
[36m|:-------------:|:-------------|:-----------:|:-------------|:----------:|:-------------|
[36m| General trash | 3468         |    Paper    | 5511         | Paper pack | 800          |
[36m|     Metal     | 827          |    Glass    | 900          |  Plastic   | 2591         |
[36m|   Styrofoam   | 1140         | Plastic bag | 4563         |  Battery   | 148          |
[36m|   Clothing    | 416          |             |              |            |              |
[36m|     total     | 20364        |             |              |            |              |
[32m[03/27 02:06:54 d2.data.build]: [39mUsing training sampler RepeatFactorTrainingSampler
[32m[03/27 02:06:54 d2.data.common]: [39mSerializing 4280 elements to byte tensors and concatenating them all ...
[32m[03/27 02:06:54 d2.data.common]: [39mSerialized dataset takes 1.92 MiB
expected_results:
[]
[32m[03/27 02:06:55 d2.engine.train_loop]: [39mStarting training from iteration 0
/opt/ml/detection/baseline/detectron2/detectron2/modeling/roi_heads/fast_rcnn.py:103: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  num_fg = fg_inds.nonzero().numel()
/opt/conda/envs/detection/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[32m[03/27 02:06:57 d2.utils.events]: [39m eta: 8:32:22  iter: 4  total_loss: 4.03  loss_cls: 2.267  loss_box_reg: 0.619  loss_rpn_cls: 1.084  loss_rpn_loc: 0.0558  time: 0.2857  data_time: 0.0781  lr: 4.996e-06  max_mem: 1508M
[32m[03/27 02:06:59 d2.utils.events]: [39m eta: 7:51:33  iter: 9  total_loss: 3.778  loss_cls: 2.261  loss_box_reg: 0.4337  loss_rpn_cls: 1.017  loss_rpn_loc: 0.05426  time: 0.2721  data_time: 0.0429  lr: 9.991e-06  max_mem: 1511M
[32m[03/27 02:07:00 d2.utils.events]: [39m eta: 7:52:02  iter: 14  total_loss: 3.871  loss_cls: 2.252  loss_box_reg: 0.48  loss_rpn_cls: 0.9666  loss_rpn_loc: 0.05273  time: 0.2685  data_time: 0.0314  lr: 1.4986e-05  max_mem: 1511M
[32m[03/27 02:07:01 d2.utils.events]: [39m eta: 7:48:58  iter: 19  total_loss: 3.5  loss_cls: 2.169  loss_box_reg: 0.5188  loss_rpn_cls: 0.9393  loss_rpn_loc: 0.05242  time: 0.2657  data_time: 0.0262  lr: 1.9981e-05  max_mem: 1511M
[32m[03/27 02:07:03 d2.utils.events]: [39m eta: 7:46:54  iter: 24  total_loss: 3.393  loss_cls: 2.041  loss_box_reg: 0.5269  loss_rpn_cls: 0.8386  loss_rpn_loc: 0.04807  time: 0.2645  data_time: 0.0087  lr: 2.4976e-05  max_mem: 1511M
[32m[03/27 02:07:04 d2.utils.events]: [39m eta: 7:47:35  iter: 29  total_loss: 3.103  loss_cls: 1.795  loss_box_reg: 0.5591  loss_rpn_cls: 0.7487  loss_rpn_loc: 0.04098  time: 0.2662  data_time: 0.0087  lr: 2.9971e-05  max_mem: 1511M
[32m[03/27 02:07:06 d2.utils.events]: [39m eta: 7:51:56  iter: 34  total_loss: 2.932  loss_cls: 1.524  loss_box_reg: 0.5918  loss_rpn_cls: 0.6621  loss_rpn_loc: 0.03463  time: 0.2708  data_time: 0.0097  lr: 3.4966e-05  max_mem: 1511M
[32m[03/27 02:07:07 d2.utils.events]: [39m eta: 7:54:17  iter: 39  total_loss: 2.442  loss_cls: 1.26  loss_box_reg: 0.6082  loss_rpn_cls: 0.5076  loss_rpn_loc: 0.02901  time: 0.2713  data_time: 0.0098  lr: 3.9961e-05  max_mem: 1511M
[32m[03/27 02:07:08 d2.utils.events]: [39m eta: 7:56:50  iter: 44  total_loss: 2.254  loss_cls: 1.076  loss_box_reg: 0.637  loss_rpn_cls: 0.4594  loss_rpn_loc: 0.02825  time: 0.2725  data_time: 0.0097  lr: 4.4956e-05  max_mem: 1511M
[32m[03/27 02:07:10 d2.utils.events]: [39m eta: 7:56:06  iter: 49  total_loss: 2.159  loss_cls: 0.9672  loss_box_reg: 0.6665  loss_rpn_cls: 0.4143  loss_rpn_loc: 0.03598  time: 0.2721  data_time: 0.0098  lr: 4.9951e-05  max_mem: 1511M
[32m[03/27 02:07:11 d2.utils.events]: [39m eta: 7:56:48  iter: 54  total_loss: 2.108  loss_cls: 0.9142  loss_box_reg: 0.696  loss_rpn_cls: 0.2992  loss_rpn_loc: 0.03589  time: 0.2733  data_time: 0.0095  lr: 5.4946e-05  max_mem: 1511M
[32m[03/27 02:07:12 d2.utils.events]: [39m eta: 7:58:44  iter: 59  total_loss: 2.108  loss_cls: 0.8934  loss_box_reg: 0.7095  loss_rpn_cls: 0.2992  loss_rpn_loc: 0.03685  time: 0.2734  data_time: 0.0094  lr: 5.9941e-05  max_mem: 1511M
[32m[03/27 02:07:14 d2.utils.events]: [39m eta: 7:56:45  iter: 64  total_loss: 1.833  loss_cls: 0.8741  loss_box_reg: 0.7612  loss_rpn_cls: 0.2356  loss_rpn_loc: 0.03685  time: 0.2716  data_time: 0.0100  lr: 6.4936e-05  max_mem: 1511M
[32m[03/27 02:07:15 d2.utils.events]: [39m eta: 7:57:05  iter: 69  total_loss: 1.833  loss_cls: 0.8317  loss_box_reg: 0.7195  loss_rpn_cls: 0.2016  loss_rpn_loc: 0.03972  time: 0.2720  data_time: 0.0100  lr: 6.9931e-05  max_mem: 1511M
[32m[03/27 02:07:17 d2.utils.events]: [39m eta: 7:57:25  iter: 74  total_loss: 1.959  loss_cls: 0.8246  loss_box_reg: 0.7195  loss_rpn_cls: 0.244  loss_rpn_loc: 0.0524  time: 0.2723  data_time: 0.0116  lr: 7.4926e-05  max_mem: 1511M
[32m[03/27 02:07:18 d2.utils.events]: [39m eta: 7:58:39  iter: 79  total_loss: 1.787  loss_cls: 0.7999  loss_box_reg: 0.699  loss_rpn_cls: 0.2036  loss_rpn_loc: 0.05032  time: 0.2728  data_time: 0.0118  lr: 7.9921e-05  max_mem: 1511M
[32m[03/27 02:07:19 d2.utils.events]: [39m eta: 7:56:31  iter: 84  total_loss: 1.681  loss_cls: 0.7777  loss_box_reg: 0.6845  loss_rpn_cls: 0.176  loss_rpn_loc: 0.04908  time: 0.2716  data_time: 0.0112  lr: 8.4916e-05  max_mem: 1511M
[32m[03/27 02:07:21 d2.utils.events]: [39m eta: 7:54:04  iter: 89  total_loss: 1.735  loss_cls: 0.7906  loss_box_reg: 0.6899  loss_rpn_cls: 0.1779  loss_rpn_loc: 0.04908  time: 0.2704  data_time: 0.0112  lr: 8.9911e-05  max_mem: 1511M
[32m[03/27 02:07:22 d2.utils.events]: [39m eta: 7:52:43  iter: 94  total_loss: 1.681  loss_cls: 0.7816  loss_box_reg: 0.6899  loss_rpn_cls: 0.1643  loss_rpn_loc: 0.03953  time: 0.2698  data_time: 0.0086  lr: 9.4906e-05  max_mem: 1513M
[32m[03/27 02:07:23 d2.utils.events]: [39m eta: 7:54:02  iter: 99  total_loss: 1.866  loss_cls: 0.7906  loss_box_reg: 0.7134  loss_rpn_cls: 0.1651  loss_rpn_loc: 0.04449  time: 0.2704  data_time: 0.0076  lr: 9.9901e-05  max_mem: 1513M
[32m[03/27 02:07:25 d2.utils.events]: [39m eta: 7:54:11  iter: 104  total_loss: 1.752  loss_cls: 0.7646  loss_box_reg: 0.6989  loss_rpn_cls: 0.1485  loss_rpn_loc: 0.048  time: 0.2705  data_time: 0.0084  lr: 0.0001049  max_mem: 1513M
[32m[03/27 02:07:26 d2.utils.events]: [39m eta: 7:53:59  iter: 109  total_loss: 1.563  loss_cls: 0.7271  loss_box_reg: 0.6964  loss_rpn_cls: 0.1102  loss_rpn_loc: 0.03479  time: 0.2699  data_time: 0.0084  lr: 0.00010989  max_mem: 1515M
[32m[03/27 02:07:27 d2.utils.events]: [39m eta: 7:53:46  iter: 114  total_loss: 1.53  loss_cls: 0.716  loss_box_reg: 0.687  loss_rpn_cls: 0.1096  loss_rpn_loc: 0.03101  time: 0.2695  data_time: 0.0084  lr: 0.00011489  max_mem: 1515M
[32m[03/27 02:07:29 d2.utils.events]: [39m eta: 7:53:45  iter: 119  total_loss: 1.483  loss_cls: 0.7065  loss_box_reg: 0.6679  loss_rpn_cls: 0.1096  loss_rpn_loc: 0.03356  time: 0.2697  data_time: 0.0099  lr: 0.00011988  max_mem: 1515M
[32m[03/27 02:07:30 d2.utils.events]: [39m eta: 7:54:05  iter: 124  total_loss: 1.502  loss_cls: 0.6999  loss_box_reg: 0.682  loss_rpn_cls: 0.103  loss_rpn_loc: 0.03101  time: 0.2700  data_time: 0.0092  lr: 0.00012488  max_mem: 1515M
[32m[03/27 02:07:31 d2.utils.events]: [39m eta: 7:54:33  iter: 129  total_loss: 1.502  loss_cls: 0.6976  loss_box_reg: 0.702  loss_rpn_cls: 0.1083  loss_rpn_loc: 0.04214  time: 0.2703  data_time: 0.0094  lr: 0.00012987  max_mem: 1515M
[32m[03/27 02:07:33 d2.utils.events]: [39m eta: 7:54:03  iter: 134  total_loss: 1.637  loss_cls: 0.7301  loss_box_reg: 0.7457  loss_rpn_cls: 0.1216  loss_rpn_loc: 0.0503  time: 0.2700  data_time: 0.0094  lr: 0.00013487  max_mem: 1515M
[32m[03/27 02:07:34 d2.utils.events]: [39m eta: 7:53:51  iter: 139  total_loss: 1.603  loss_cls: 0.7354  loss_box_reg: 0.7413  loss_rpn_cls: 0.1083  loss_rpn_loc: 0.03891  time: 0.2696  data_time: 0.0079  lr: 0.00013986  max_mem: 1515M
[32m[03/27 02:07:35 d2.utils.events]: [39m eta: 7:53:39  iter: 144  total_loss: 1.624  loss_cls: 0.7509  loss_box_reg: 0.7348  loss_rpn_cls: 0.1325  loss_rpn_loc: 0.04107  time: 0.2693  data_time: 0.0094  lr: 0.00014486  max_mem: 1515M
[32m[03/27 02:07:37 d2.utils.events]: [39m eta: 7:53:37  iter: 149  total_loss: 1.646  loss_cls: 0.7381  loss_box_reg: 0.7348  loss_rpn_cls: 0.1325  loss_rpn_loc: 0.03373  time: 0.2694  data_time: 0.0095  lr: 0.00014985  max_mem: 1515M
[32m[03/27 02:07:38 d2.utils.events]: [39m eta: 7:51:57  iter: 154  total_loss: 1.581  loss_cls: 0.7136  loss_box_reg: 0.7067  loss_rpn_cls: 0.1052  loss_rpn_loc: 0.02868  time: 0.2669  data_time: 0.0100  lr: 0.00015485  max_mem: 1515M
[32m[03/27 02:07:39 d2.utils.events]: [39m eta: 7:51:15  iter: 159  total_loss: 1.633  loss_cls: 0.7077  loss_box_reg: 0.7322  loss_rpn_cls: 0.1052  loss_rpn_loc: 0.03365  time: 0.2639  data_time: 0.0101  lr: 0.00015984  max_mem: 1516M
[32m[03/27 02:07:39 d2.utils.events]: [39m eta: 7:50:03  iter: 164  total_loss: 1.672  loss_cls: 0.7292  loss_box_reg: 0.7447  loss_rpn_cls: 0.102  loss_rpn_loc: 0.04042  time: 0.2610  data_time: 0.0085  lr: 0.00016484  max_mem: 1516M
[32m[03/27 02:07:40 d2.utils.events]: [39m eta: 7:48:42  iter: 169  total_loss: 1.698  loss_cls: 0.7486  loss_box_reg: 0.7487  loss_rpn_cls: 0.1183  loss_rpn_loc: 0.03994  time: 0.2584  data_time: 0.0085  lr: 0.00016983  max_mem: 1516M
[32m[03/27 02:07:41 d2.utils.events]: [39m eta: 7:47:38  iter: 174  total_loss: 1.729  loss_cls: 0.7713  loss_box_reg: 0.7968  loss_rpn_cls: 0.1394  loss_rpn_loc: 0.04883  time: 0.2559  data_time: 0.0080  lr: 0.00017483  max_mem: 1516M
[32m[03/27 02:07:42 d2.utils.events]: [39m eta: 7:46:27  iter: 179  total_loss: 1.729  loss_cls: 0.7713  loss_box_reg: 0.7803  loss_rpn_cls: 0.1213  loss_rpn_loc: 0.03164  time: 0.2535  data_time: 0.0079  lr: 0.00017982  max_mem: 1516M
[32m[03/27 02:07:43 d2.utils.events]: [39m eta: 7:45:28  iter: 184  total_loss: 1.616  loss_cls: 0.7205  loss_box_reg: 0.7288  loss_rpn_cls: 0.1106  loss_rpn_loc: 0.02988  time: 0.2513  data_time: 0.0079  lr: 0.00018482  max_mem: 1516M
[32m[03/27 02:07:44 d2.utils.events]: [39m eta: 7:43:46  iter: 189  total_loss: 1.586  loss_cls: 0.7205  loss_box_reg: 0.7105  loss_rpn_cls: 0.09862  loss_rpn_loc: 0.02817  time: 0.2491  data_time: 0.0080  lr: 0.00018981  max_mem: 1516M
[32m[03/27 02:07:45 d2.utils.events]: [39m eta: 7:41:52  iter: 194  total_loss: 1.616  loss_cls: 0.7205  loss_box_reg: 0.7105  loss_rpn_cls: 0.09162  loss_rpn_loc: 0.02813  time: 0.2471  data_time: 0.0080  lr: 0.00019481  max_mem: 1516M
[32m[03/27 02:07:45 d2.utils.events]: [39m eta: 7:40:56  iter: 199  total_loss: 1.645  loss_cls: 0.7497  loss_box_reg: 0.7331  loss_rpn_cls: 0.1174  loss_rpn_loc: 0.04067  time: 0.2452  data_time: 0.0080  lr: 0.0001998  max_mem: 1516M
[32m[03/27 02:07:46 d2.utils.events]: [39m eta: 7:39:44  iter: 204  total_loss: 1.689  loss_cls: 0.7546  loss_box_reg: 0.7711  loss_rpn_cls: 0.1286  loss_rpn_loc: 0.03654  time: 0.2434  data_time: 0.0080  lr: 0.0002048  max_mem: 1516M
[32m[03/27 02:07:47 d2.utils.events]: [39m eta: 7:38:03  iter: 209  total_loss: 1.668  loss_cls: 0.7434  loss_box_reg: 0.7479  loss_rpn_cls: 0.1319  loss_rpn_loc: 0.03503  time: 0.2416  data_time: 0.0080  lr: 0.00020979  max_mem: 1516M
[32m[03/27 02:07:48 d2.utils.events]: [39m eta: 7:36:36  iter: 214  total_loss: 1.615  loss_cls: 0.7313  loss_box_reg: 0.7606  loss_rpn_cls: 0.1319  loss_rpn_loc: 0.03098  time: 0.2400  data_time: 0.0079  lr: 0.00021479  max_mem: 1516M
[32m[03/27 02:07:49 d2.utils.events]: [39m eta: 7:35:50  iter: 219  total_loss: 1.582  loss_cls: 0.6853  loss_box_reg: 0.7348  loss_rpn_cls: 0.1289  loss_rpn_loc: 0.03098  time: 0.2388  data_time: 0.0079  lr: 0.00021978  max_mem: 1516M
[32m[03/27 02:07:50 d2.utils.events]: [39m eta: 7:33:58  iter: 224  total_loss: 1.556  loss_cls: 0.6944  loss_box_reg: 0.7391  loss_rpn_cls: 0.09179  loss_rpn_loc: 0.03041  time: 0.2373  data_time: 0.0079  lr: 0.00022478  max_mem: 1516M
[32m[03/27 02:07:51 d2.utils.events]: [39m eta: 7:33:08  iter: 229  total_loss: 1.615  loss_cls: 0.6849  loss_box_reg: 0.7408  loss_rpn_cls: 0.1391  loss_rpn_loc: 0.03559  time: 0.2359  data_time: 0.0079  lr: 0.00022977  max_mem: 1516M
[32m[03/27 02:07:52 d2.utils.events]: [39m eta: 7:32:07  iter: 234  total_loss: 1.572  loss_cls: 0.6835  loss_box_reg: 0.7046  loss_rpn_cls: 0.1409  loss_rpn_loc: 0.04567  time: 0.2345  data_time: 0.0078  lr: 0.00023477  max_mem: 1516M
[32m[03/27 02:07:52 d2.utils.events]: [39m eta: 7:30:52  iter: 239  total_loss: 1.565  loss_cls: 0.6835  loss_box_reg: 0.7046  loss_rpn_cls: 0.1109  loss_rpn_loc: 0.03339  time: 0.2331  data_time: 0.0078  lr: 0.00023976  max_mem: 1516M
[32m[03/27 02:07:53 d2.utils.events]: [39m eta: 7:28:55  iter: 244  total_loss: 1.565  loss_cls: 0.6784  loss_box_reg: 0.7046  loss_rpn_cls: 0.1158  loss_rpn_loc: 0.03971  time: 0.2318  data_time: 0.0077  lr: 0.00024476  max_mem: 1516M
[32m[03/27 02:07:54 d2.utils.events]: [39m eta: 7:28:04  iter: 249  total_loss: 1.567  loss_cls: 0.6872  loss_box_reg: 0.7297  loss_rpn_cls: 0.09119  loss_rpn_loc: 0.03339  time: 0.2307  data_time: 0.0083  lr: 0.00024975  max_mem: 1516M
[32m[03/27 02:07:55 d2.utils.events]: [39m eta: 7:26:27  iter: 254  total_loss: 1.658  loss_cls: 0.7156  loss_box_reg: 0.7603  loss_rpn_cls: 0.07145  loss_rpn_loc: 0.02109  time: 0.2295  data_time: 0.0085  lr: 0.00025475  max_mem: 1516M
[32m[03/27 02:07:56 d2.utils.events]: [39m eta: 7:25:59  iter: 259  total_loss: 1.727  loss_cls: 0.7466  loss_box_reg: 0.7664  loss_rpn_cls: 0.08548  loss_rpn_loc: 0.03179  time: 0.2285  data_time: 0.0087  lr: 0.00025974  max_mem: 1516M
[32m[03/27 02:07:57 d2.utils.events]: [39m eta: 7:24:32  iter: 264  total_loss: 1.77  loss_cls: 0.7789  loss_box_reg: 0.7767  loss_rpn_cls: 0.1058  loss_rpn_loc: 0.03966  time: 0.2274  data_time: 0.0093  lr: 0.00026474  max_mem: 1517M
[32m[03/27 02:07:58 d2.utils.events]: [39m eta: 7:21:32  iter: 269  total_loss: 1.675  loss_cls: 0.7018  loss_box_reg: 0.743  loss_rpn_cls: 0.1312  loss_rpn_loc: 0.04715  time: 0.2264  data_time: 0.0087  lr: 0.00026973  max_mem: 1517M
[32m[03/27 02:07:59 d2.utils.events]: [39m eta: 7:19:34  iter: 274  total_loss: 1.675  loss_cls: 0.6921  loss_box_reg: 0.7226  loss_rpn_cls: 0.1388  loss_rpn_loc: 0.05166  time: 0.2253  data_time: 0.0084  lr: 0.00027473  max_mem: 1517M
[32m[03/27 02:07:59 d2.utils.events]: [39m eta: 7:18:13  iter: 279  total_loss: 1.631  loss_cls: 0.6817  loss_box_reg: 0.7132  loss_rpn_cls: 0.1165  loss_rpn_loc: 0.04886  time: 0.2242  data_time: 0.0081  lr: 0.00027972  max_mem: 1517M
[32m[03/27 02:08:00 d2.utils.events]: [39m eta: 7:16:03  iter: 284  total_loss: 1.578  loss_cls: 0.6642  loss_box_reg: 0.7176  loss_rpn_cls: 0.09845  loss_rpn_loc: 0.04723  time: 0.2233  data_time: 0.0075  lr: 0.00028472  max_mem: 1517M
[32m[03/27 02:08:01 d2.utils.events]: [39m eta: 7:11:00  iter: 289  total_loss: 1.631  loss_cls: 0.7183  loss_box_reg: 0.7461  loss_rpn_cls: 0.09822  loss_rpn_loc: 0.04738  time: 0.2223  data_time: 0.0074  lr: 0.00028971  max_mem: 1517M
[32m[03/27 02:08:02 d2.utils.events]: [39m eta: 7:02:57  iter: 294  total_loss: 1.696  loss_cls: 0.7519  loss_box_reg: 0.7461  loss_rpn_cls: 0.1031  loss_rpn_loc: 0.05005  time: 0.2214  data_time: 0.0076  lr: 0.00029471  max_mem: 1517M
[32m[03/27 02:08:03 d2.utils.events]: [39m eta: 6:35:27  iter: 299  total_loss: 1.655  loss_cls: 0.7463  loss_box_reg: 0.7461  loss_rpn_cls: 0.1167  loss_rpn_loc: 0.05485  time: 0.2205  data_time: 0.0076  lr: 0.0002997  max_mem: 1517M
[32m[03/27 02:08:04 d2.utils.events]: [39m eta: 5:27:35  iter: 304  total_loss: 1.627  loss_cls: 0.6968  loss_box_reg: 0.7357  loss_rpn_cls: 0.1022  loss_rpn_loc: 0.05395  time: 0.2197  data_time: 0.0076  lr: 0.0003047  max_mem: 1517M
[32m[03/27 02:08:05 d2.utils.events]: [39m eta: 5:19:35  iter: 309  total_loss: 1.574  loss_cls: 0.6682  loss_box_reg: 0.6818  loss_rpn_cls: 0.09835  loss_rpn_loc: 0.04972  time: 0.2190  data_time: 0.0078  lr: 0.00030969  max_mem: 1517M
[32m[03/27 02:08:05 d2.utils.events]: [39m eta: 5:15:47  iter: 314  total_loss: 1.574  loss_cls: 0.6701  loss_box_reg: 0.6818  loss_rpn_cls: 0.1071  loss_rpn_loc: 0.04505  time: 0.2182  data_time: 0.0079  lr: 0.00031469  max_mem: 1517M
[32m[03/27 02:08:06 d2.utils.events]: [39m eta: 5:14:03  iter: 319  total_loss: 1.475  loss_cls: 0.6663  loss_box_reg: 0.658  loss_rpn_cls: 0.1148  loss_rpn_loc: 0.04505  time: 0.2175  data_time: 0.0080  lr: 0.00031968  max_mem: 1517M
[32m[03/27 02:08:07 d2.utils.events]: [39m eta: 5:12:32  iter: 324  total_loss: 1.482  loss_cls: 0.6358  loss_box_reg: 0.6615  loss_rpn_cls: 0.1267  loss_rpn_loc: 0.04824  time: 0.2167  data_time: 0.0079  lr: 0.00032468  max_mem: 1517M
[32m[03/27 02:08:08 d2.utils.events]: [39m eta: 5:11:48  iter: 329  total_loss: 1.452  loss_cls: 0.6203  loss_box_reg: 0.6615  loss_rpn_cls: 0.1103  loss_rpn_loc: 0.03415  time: 0.2160  data_time: 0.0079  lr: 0.00032967  max_mem: 1517M
[32m[03/27 02:08:09 d2.utils.events]: [39m eta: 5:10:12  iter: 334  total_loss: 1.368  loss_cls: 0.5886  loss_box_reg: 0.6615  loss_rpn_cls: 0.08041  loss_rpn_loc: 0.02175  time: 0.2153  data_time: 0.0077  lr: 0.00033467  max_mem: 1517M
[32m[03/27 02:08:10 d2.utils.events]: [39m eta: 5:09:33  iter: 339  total_loss: 1.349  loss_cls: 0.6145  loss_box_reg: 0.6704  loss_rpn_cls: 0.06231  loss_rpn_loc: 0.02207  time: 0.2146  data_time: 0.0076  lr: 0.00033966  max_mem: 1517M
[32m[03/27 02:08:11 d2.utils.events]: [39m eta: 5:08:57  iter: 344  total_loss: 1.304  loss_cls: 0.6145  loss_box_reg: 0.6838  loss_rpn_cls: 0.0569  loss_rpn_loc: 0.01414  time: 0.2139  data_time: 0.0076  lr: 0.00034466  max_mem: 1517M
[32m[03/27 02:08:11 d2.utils.events]: [39m eta: 5:08:07  iter: 349  total_loss: 1.303  loss_cls: 0.6145  loss_box_reg: 0.6855  loss_rpn_cls: 0.0569  loss_rpn_loc: 0.01701  time: 0.2133  data_time: 0.0074  lr: 0.00034965  max_mem: 1517M
[32m[03/27 02:08:12 d2.utils.events]: [39m eta: 5:07:45  iter: 354  total_loss: 1.304  loss_cls: 0.6193  loss_box_reg: 0.6377  loss_rpn_cls: 0.06739  loss_rpn_loc: 0.02438  time: 0.2126  data_time: 0.0074  lr: 0.00035465  max_mem: 1517M
[32m[03/27 02:08:13 d2.utils.events]: [39m eta: 5:07:21  iter: 359  total_loss: 1.415  loss_cls: 0.6356  loss_box_reg: 0.6595  loss_rpn_cls: 0.08722  loss_rpn_loc: 0.02854  time: 0.2120  data_time: 0.0074  lr: 0.00035964  max_mem: 1517M
[32m[03/27 02:08:14 d2.utils.events]: [39m eta: 5:07:20  iter: 364  total_loss: 1.551  loss_cls: 0.6703  loss_box_reg: 0.6654  loss_rpn_cls: 0.1014  loss_rpn_loc: 0.0477  time: 0.2115  data_time: 0.0076  lr: 0.00036464  max_mem: 1522M
[32m[03/27 02:08:15 d2.utils.events]: [39m eta: 5:07:50  iter: 369  total_loss: 1.551  loss_cls: 0.6703  loss_box_reg: 0.6498  loss_rpn_cls: 0.1057  loss_rpn_loc: 0.0611  time: 0.2110  data_time: 0.0079  lr: 0.00036963  max_mem: 1522M
[32m[03/27 02:08:16 d2.utils.events]: [39m eta: 5:07:19  iter: 374  total_loss: 1.459  loss_cls: 0.6524  loss_box_reg: 0.6464  loss_rpn_cls: 0.1046  loss_rpn_loc: 0.05786  time: 0.2105  data_time: 0.0081  lr: 0.00037463  max_mem: 1522M
[32m[03/27 02:08:17 d2.utils.events]: [39m eta: 5:07:01  iter: 379  total_loss: 1.435  loss_cls: 0.6531  loss_box_reg: 0.6464  loss_rpn_cls: 0.1076  loss_rpn_loc: 0.05082  time: 0.2099  data_time: 0.0081  lr: 0.00037962  max_mem: 1522M
[32m[03/27 02:08:17 d2.utils.events]: [39m eta: 5:06:55  iter: 384  total_loss: 1.426  loss_cls: 0.6531  loss_box_reg: 0.6345  loss_rpn_cls: 0.09584  loss_rpn_loc: 0.02706  time: 0.2094  data_time: 0.0080  lr: 0.00038462  max_mem: 1522M
[32m[03/27 02:08:18 d2.utils.events]: [39m eta: 5:06:48  iter: 389  total_loss: 1.462  loss_cls: 0.6622  loss_box_reg: 0.6564  loss_rpn_cls: 0.09505  loss_rpn_loc: 0.02706  time: 0.2089  data_time: 0.0078  lr: 0.00038961  max_mem: 1522M
[32m[03/27 02:08:19 d2.utils.events]: [39m eta: 5:06:34  iter: 394  total_loss: 1.579  loss_cls: 0.7317  loss_box_reg: 0.6677  loss_rpn_cls: 0.1309  loss_rpn_loc: 0.03599  time: 0.2084  data_time: 0.0077  lr: 0.00039461  max_mem: 1522M
[32m[03/27 02:08:20 d2.utils.events]: [39m eta: 5:06:39  iter: 399  total_loss: 1.595  loss_cls: 0.7408  loss_box_reg: 0.6778  loss_rpn_cls: 0.1243  loss_rpn_loc: 0.03825  time: 0.2080  data_time: 0.0088  lr: 0.0003996  max_mem: 1522M
[32m[03/27 02:08:21 d2.utils.events]: [39m eta: 5:06:29  iter: 404  total_loss: 1.49  loss_cls: 0.7168  loss_box_reg: 0.6642  loss_rpn_cls: 0.1243  loss_rpn_loc: 0.04473  time: 0.2076  data_time: 0.0089  lr: 0.0004046  max_mem: 1522M
[32m[03/27 02:08:22 d2.utils.events]: [39m eta: 5:06:28  iter: 409  total_loss: 1.446  loss_cls: 0.6743  loss_box_reg: 0.6285  loss_rpn_cls: 0.1121  loss_rpn_loc: 0.03498  time: 0.2072  data_time: 0.0092  lr: 0.00040959  max_mem: 1522M
[32m[03/27 02:08:23 d2.utils.events]: [39m eta: 5:06:27  iter: 414  total_loss: 1.417  loss_cls: 0.6399  loss_box_reg: 0.6138  loss_rpn_cls: 0.08782  loss_rpn_loc: 0.02423  time: 0.2069  data_time: 0.0106  lr: 0.00041459  max_mem: 1522M
[32m[03/27 02:08:24 d2.utils.events]: [39m eta: 5:06:16  iter: 419  total_loss: 1.41  loss_cls: 0.6471  loss_box_reg: 0.619  loss_rpn_cls: 0.09319  loss_rpn_loc: 0.02544  time: 0.2065  data_time: 0.0095  lr: 0.00041958  max_mem: 1522M
[32m[03/27 02:08:24 d2.utils.events]: [39m eta: 5:06:05  iter: 424  total_loss: 1.34  loss_cls: 0.6361  loss_box_reg: 0.5957  loss_rpn_cls: 0.1047  loss_rpn_loc: 0.03704  time: 0.2060  data_time: 0.0095  lr: 0.00042458  max_mem: 1522M
[32m[03/27 02:08:25 d2.utils.events]: [39m eta: 5:05:46  iter: 429  total_loss: 1.335  loss_cls: 0.6047  loss_box_reg: 0.5626  loss_rpn_cls: 0.1123  loss_rpn_loc: 0.04847  time: 0.2056  data_time: 0.0092  lr: 0.00042957  max_mem: 1522M
[32m[03/27 02:08:26 d2.utils.events]: [39m eta: 5:05:37  iter: 434  total_loss: 1.263  loss_cls: 0.5868  loss_box_reg: 0.5317  loss_rpn_cls: 0.1134  loss_rpn_loc: 0.04901  time: 0.2052  data_time: 0.0081  lr: 0.00043457  max_mem: 1522M
[32m[03/27 02:08:27 d2.utils.events]: [39m eta: 5:05:32  iter: 439  total_loss: 1.207  loss_cls: 0.5661  loss_box_reg: 0.506  loss_rpn_cls: 0.1134  loss_rpn_loc: 0.04691  time: 0.2048  data_time: 0.0081  lr: 0.00043956  max_mem: 1522M
[32m[03/27 02:08:28 d2.utils.events]: [39m eta: 5:05:31  iter: 444  total_loss: 1.246  loss_cls: 0.5945  loss_box_reg: 0.5452  loss_rpn_cls: 0.09858  loss_rpn_loc: 0.03143  time: 0.2045  data_time: 0.0081  lr: 0.00044456  max_mem: 1522M
[32m[03/27 02:08:29 d2.utils.events]: [39m eta: 5:05:22  iter: 449  total_loss: 1.319  loss_cls: 0.606  loss_box_reg: 0.5519  loss_rpn_cls: 0.08336  loss_rpn_loc: 0.02585  time: 0.2041  data_time: 0.0080  lr: 0.00044955  max_mem: 1522M
[32m[03/27 02:08:30 d2.utils.events]: [39m eta: 5:05:11  iter: 454  total_loss: 1.319  loss_cls: 0.629  loss_box_reg: 0.5519  loss_rpn_cls: 0.08076  loss_rpn_loc: 0.02548  time: 0.2037  data_time: 0.0078  lr: 0.00045455  max_mem: 1522M
[32m[03/27 02:08:31 d2.utils.events]: [39m eta: 5:05:13  iter: 459  total_loss: 1.333  loss_cls: 0.6375  loss_box_reg: 0.5605  loss_rpn_cls: 0.0912  loss_rpn_loc: 0.02548  time: 0.2034  data_time: 0.0078  lr: 0.00045954  max_mem: 1522M
[32m[03/27 02:08:31 d2.utils.events]: [39m eta: 5:05:09  iter: 464  total_loss: 1.366  loss_cls: 0.6375  loss_box_reg: 0.5621  loss_rpn_cls: 0.1181  loss_rpn_loc: 0.04289  time: 0.2031  data_time: 0.0086  lr: 0.00046454  max_mem: 1522M
[32m[03/27 02:08:32 d2.utils.events]: [39m eta: 5:05:12  iter: 469  total_loss: 1.348  loss_cls: 0.6104  loss_box_reg: 0.5357  loss_rpn_cls: 0.1101  loss_rpn_loc: 0.03887  time: 0.2028  data_time: 0.0093  lr: 0.00046953  max_mem: 1522M
[32m[03/27 02:08:33 d2.utils.events]: [39m eta: 5:05:14  iter: 474  total_loss: 1.302  loss_cls: 0.6084  loss_box_reg: 0.5297  loss_rpn_cls: 0.1089  loss_rpn_loc: 0.03442  time: 0.2028  data_time: 0.0098  lr: 0.00047453  max_mem: 1522M
[32m[03/27 02:08:34 d2.utils.events]: [39m eta: 5:05:17  iter: 479  total_loss: 1.244  loss_cls: 0.596  loss_box_reg: 0.5426  loss_rpn_cls: 0.06844  loss_rpn_loc: 0.02903  time: 0.2025  data_time: 0.0099  lr: 0.00047952  max_mem: 1522M
[32m[03/27 02:08:35 d2.utils.events]: [39m eta: 5:05:12  iter: 484  total_loss: 1.265  loss_cls: 0.6089  loss_box_reg: 0.5156  loss_rpn_cls: 0.06696  loss_rpn_loc: 0.02903  time: 0.2021  data_time: 0.0092  lr: 0.00048452  max_mem: 1522M
[32m[03/27 02:08:36 d2.utils.events]: [39m eta: 5:05:13  iter: 489  total_loss: 1.233  loss_cls: 0.6022  loss_box_reg: 0.4872  loss_rpn_cls: 0.06396  loss_rpn_loc: 0.02892  time: 0.2019  data_time: 0.0086  lr: 0.00048951  max_mem: 1522M
[32m[03/27 02:08:37 d2.utils.events]: [39m eta: 5:05:04  iter: 494  total_loss: 1.265  loss_cls: 0.6232  loss_box_reg: 0.4803  loss_rpn_cls: 0.07851  loss_rpn_loc: 0.02938  time: 0.2015  data_time: 0.0081  lr: 0.00049451  max_mem: 1522M
[32m[03/27 02:08:38 d2.utils.events]: [39m eta: 5:04:52  iter: 499  total_loss: 1.349  loss_cls: 0.6665  loss_box_reg: 0.4774  loss_rpn_cls: 0.0896  loss_rpn_loc: 0.04195  time: 0.2012  data_time: 0.0080  lr: 0.0004995  max_mem: 1522M
[32m[03/27 02:08:39 d2.utils.events]: [39m eta: 5:04:49  iter: 504  total_loss: 1.225  loss_cls: 0.5906  loss_box_reg: 0.4714  loss_rpn_cls: 0.08901  loss_rpn_loc: 0.03692  time: 0.2009  data_time: 0.0079  lr: 0.0005045  max_mem: 1522M
[32m[03/27 02:08:39 d2.utils.events]: [39m eta: 5:04:41  iter: 509  total_loss: 1.225  loss_cls: 0.5906  loss_box_reg: 0.4549  loss_rpn_cls: 0.07526  loss_rpn_loc: 0.02007  time: 0.2006  data_time: 0.0078  lr: 0.00050949  max_mem: 1522M
[32m[03/27 02:08:40 d2.utils.events]: [39m eta: 5:04:39  iter: 514  total_loss: 1.136  loss_cls: 0.5563  loss_box_reg: 0.4428  loss_rpn_cls: 0.0798  loss_rpn_loc: 0.02356  time: 0.2003  data_time: 0.0078  lr: 0.00051449  max_mem: 1522M
[32m[03/27 02:08:41 d2.utils.events]: [39m eta: 5:04:36  iter: 519  total_loss: 1.101  loss_cls: 0.5563  loss_box_reg: 0.4332  loss_rpn_cls: 0.06883  loss_rpn_loc: 0.02225  time: 0.2000  data_time: 0.0079  lr: 0.00051948  max_mem: 1522M
[32m[03/27 02:08:42 d2.utils.events]: [39m eta: 5:04:32  iter: 524  total_loss: 1.2  loss_cls: 0.6008  loss_box_reg: 0.4766  loss_rpn_cls: 0.0903  loss_rpn_loc: 0.02926  time: 0.1998  data_time: 0.0079  lr: 0.00052448  max_mem: 1522M
[32m[03/27 02:08:43 d2.utils.events]: [39m eta: 5:04:29  iter: 529  total_loss: 1.332  loss_cls: 0.6312  loss_box_reg: 0.5215  loss_rpn_cls: 0.1003  loss_rpn_loc: 0.03496  time: 0.1995  data_time: 0.0079  lr: 0.00052947  max_mem: 1522M
[32m[03/27 02:08:44 d2.utils.events]: [39m eta: 5:04:30  iter: 534  total_loss: 1.3  loss_cls: 0.6286  loss_box_reg: 0.5508  loss_rpn_cls: 0.08392  loss_rpn_loc: 0.03126  time: 0.1993  data_time: 0.0079  lr: 0.00053447  max_mem: 1522M
[32m[03/27 02:08:45 d2.utils.events]: [39m eta: 5:04:27  iter: 539  total_loss: 1.213  loss_cls: 0.6098  loss_box_reg: 0.5063  loss_rpn_cls: 0.0854  loss_rpn_loc: 0.03298  time: 0.1990  data_time: 0.0079  lr: 0.00053946  max_mem: 1522M
[32m[03/27 02:08:45 d2.utils.events]: [39m eta: 5:04:22  iter: 544  total_loss: 1.184  loss_cls: 0.5991  loss_box_reg: 0.4603  loss_rpn_cls: 0.07734  loss_rpn_loc: 0.02819  time: 0.1987  data_time: 0.0080  lr: 0.00054446  max_mem: 1522M
[32m[03/27 02:08:46 d2.utils.events]: [39m eta: 5:04:22  iter: 549  total_loss: 1.179  loss_cls: 0.6022  loss_box_reg: 0.4377  loss_rpn_cls: 0.07734  loss_rpn_loc: 0.03143  time: 0.1985  data_time: 0.0080  lr: 0.00054945  max_mem: 1522M
[32m[03/27 02:08:47 d2.utils.events]: [39m eta: 5:04:20  iter: 554  total_loss: 1.168  loss_cls: 0.5927  loss_box_reg: 0.4436  loss_rpn_cls: 0.07734  loss_rpn_loc: 0.0331  time: 0.1983  data_time: 0.0080  lr: 0.00055445  max_mem: 1522M
[32m[03/27 02:08:48 d2.utils.events]: [39m eta: 5:04:16  iter: 559  total_loss: 1.188  loss_cls: 0.6175  loss_box_reg: 0.461  loss_rpn_cls: 0.07535  loss_rpn_loc: 0.0299  time: 0.1980  data_time: 0.0079  lr: 0.00055944  max_mem: 1522M
[32m[03/27 02:08:49 d2.utils.events]: [39m eta: 5:04:14  iter: 564  total_loss: 1.218  loss_cls: 0.633  loss_box_reg: 0.4689  loss_rpn_cls: 0.08173  loss_rpn_loc: 0.03451  time: 0.1978  data_time: 0.0079  lr: 0.00056444  max_mem: 1522M
[32m[03/27 02:08:50 d2.utils.events]: [39m eta: 5:04:12  iter: 569  total_loss: 1.21  loss_cls: 0.6216  loss_box_reg: 0.4662  loss_rpn_cls: 0.08036  loss_rpn_loc: 0.02984  time: 0.1975  data_time: 0.0079  lr: 0.00056943  max_mem: 1522M
[32m[03/27 02:08:51 d2.utils.events]: [39m eta: 5:04:10  iter: 574  total_loss: 1.191  loss_cls: 0.5829  loss_box_reg: 0.4339  loss_rpn_cls: 0.08036  loss_rpn_loc: 0.03004  time: 0.1973  data_time: 0.0079  lr: 0.00057443  max_mem: 1522M
[32m[03/27 02:08:52 d2.utils.events]: [39m eta: 5:04:11  iter: 579  total_loss: 1.074  loss_cls: 0.566  loss_box_reg: 0.4269  loss_rpn_cls: 0.07204  loss_rpn_loc: 0.02272  time: 0.1971  data_time: 0.0086  lr: 0.00057942  max_mem: 1522M
[32m[03/27 02:08:52 d2.utils.events]: [39m eta: 5:04:10  iter: 584  total_loss: 1.045  loss_cls: 0.504  loss_box_reg: 0.396  loss_rpn_cls: 0.06447  loss_rpn_loc: 0.01795  time: 0.1969  data_time: 0.0087  lr: 0.00058442  max_mem: 1522M
[32m[03/27 02:08:53 d2.utils.events]: [39m eta: 5:04:10  iter: 589  total_loss: 1.074  loss_cls: 0.504  loss_box_reg: 0.4063  loss_rpn_cls: 0.06809  loss_rpn_loc: 0.03014  time: 0.1967  data_time: 0.0089  lr: 0.00058941  max_mem: 1522M
[32m[03/27 02:08:54 d2.utils.events]: [39m eta: 5:04:09  iter: 594  total_loss: 1.073  loss_cls: 0.5011  loss_box_reg: 0.4174  loss_rpn_cls: 0.06809  loss_rpn_loc: 0.02798  time: 0.1965  data_time: 0.0090  lr: 0.00059441  max_mem: 1522M
[32m[03/27 02:08:55 d2.utils.events]: [39m eta: 5:04:06  iter: 599  total_loss: 1.038  loss_cls: 0.4842  loss_box_reg: 0.4154  loss_rpn_cls: 0.0722  loss_rpn_loc: 0.02798  time: 0.1963  data_time: 0.0082  lr: 0.0005994  max_mem: 1522M
[32m[03/27 02:08:56 d2.utils.events]: [39m eta: 5:04:06  iter: 604  total_loss: 1.038  loss_cls: 0.5182  loss_box_reg: 0.4457  loss_rpn_cls: 0.07143  loss_rpn_loc: 0.02773  time: 0.1961  data_time: 0.0089  lr: 0.0006044  max_mem: 1522M
[32m[03/27 02:08:57 d2.utils.events]: [39m eta: 5:04:05  iter: 609  total_loss: 1.038  loss_cls: 0.5204  loss_box_reg: 0.4259  loss_rpn_cls: 0.07532  loss_rpn_loc: 0.02456  time: 0.1959  data_time: 0.0087  lr: 0.00060939  max_mem: 1522M
[32m[03/27 02:08:58 d2.utils.events]: [39m eta: 5:04:00  iter: 614  total_loss: 1.067  loss_cls: 0.5534  loss_box_reg: 0.4388  loss_rpn_cls: 0.07532  loss_rpn_loc: 0.02993  time: 0.1957  data_time: 0.0086  lr: 0.00061439  max_mem: 1522M
[32m[03/27 02:08:59 d2.utils.events]: [39m eta: 5:03:58  iter: 619  total_loss: 1.266  loss_cls: 0.6245  loss_box_reg: 0.4467  loss_rpn_cls: 0.08404  loss_rpn_loc: 0.03564  time: 0.1956  data_time: 0.0087  lr: 0.00061938  max_mem: 1522M
[32m[03/27 02:08:59 d2.utils.events]: [39m eta: 5:04:02  iter: 624  total_loss: 1.246  loss_cls: 0.6462  loss_box_reg: 0.4467  loss_rpn_cls: 0.08907  loss_rpn_loc: 0.03683  time: 0.1954  data_time: 0.0086  lr: 0.00062438  max_mem: 1522M
[32m[03/27 02:09:00 d2.utils.events]: [39m eta: 5:04:02  iter: 629  total_loss: 1.274  loss_cls: 0.6494  loss_box_reg: 0.4662  loss_rpn_cls: 0.1059  loss_rpn_loc: 0.0509  time: 0.1953  data_time: 0.0090  lr: 0.00062937  max_mem: 1522M
[32m[03/27 02:09:01 d2.utils.events]: [39m eta: 5:04:03  iter: 634  total_loss: 1.274  loss_cls: 0.6596  loss_box_reg: 0.4614  loss_rpn_cls: 0.1176  loss_rpn_loc: 0.0509  time: 0.1952  data_time: 0.0095  lr: 0.00063437  max_mem: 1522M
[32m[03/27 02:09:02 d2.utils.events]: [39m eta: 5:04:04  iter: 639  total_loss: 1.257  loss_cls: 0.6375  loss_box_reg: 0.4356  loss_rpn_cls: 0.1176  loss_rpn_loc: 0.04629  time: 0.1951  data_time: 0.0094  lr: 0.00063936  max_mem: 1522M
[32m[03/27 02:09:03 d2.utils.events]: [39m eta: 5:04:04  iter: 644  total_loss: 1.204  loss_cls: 0.5802  loss_box_reg: 0.3969  loss_rpn_cls: 0.1059  loss_rpn_loc: 0.04473  time: 0.1949  data_time: 0.0094  lr: 0.00064436  max_mem: 1522M
[32m[03/27 02:09:04 d2.utils.events]: [39m eta: 5:04:01  iter: 649  total_loss: 1.111  loss_cls: 0.4852  loss_box_reg: 0.3695  loss_rpn_cls: 0.1166  loss_rpn_loc: 0.03856  time: 0.1947  data_time: 0.0090  lr: 0.00064935  max_mem: 1522M
[32m[03/27 02:09:05 d2.utils.events]: [39m eta: 5:03:59  iter: 654  total_loss: 1.198  loss_cls: 0.5463  loss_box_reg: 0.455  loss_rpn_cls: 0.09847  loss_rpn_loc: 0.02529  time: 0.1945  data_time: 0.0086  lr: 0.00065435  max_mem: 1522M
[32m[03/27 02:09:06 d2.utils.events]: [39m eta: 5:03:58  iter: 659  total_loss: 1.155  loss_cls: 0.5538  loss_box_reg: 0.453  loss_rpn_cls: 0.08919  loss_rpn_loc: 0.02526  time: 0.1944  data_time: 0.0087  lr: 0.00065934  max_mem: 1522M
[32m[03/27 02:09:07 d2.utils.events]: [39m eta: 5:03:59  iter: 664  total_loss: 1.287  loss_cls: 0.6645  loss_box_reg: 0.4772  loss_rpn_cls: 0.09625  loss_rpn_loc: 0.04534  time: 0.1942  data_time: 0.0084  lr: 0.00066434  max_mem: 1522M
[32m[03/27 02:09:07 d2.utils.events]: [39m eta: 5:04:01  iter: 669  total_loss: 1.28  loss_cls: 0.6645  loss_box_reg: 0.436  loss_rpn_cls: 0.08919  loss_rpn_loc: 0.03183  time: 0.1941  data_time: 0.0085  lr: 0.00066933  max_mem: 1522M
[32m[03/27 02:09:08 d2.utils.events]: [39m eta: 5:04:00  iter: 674  total_loss: 1.028  loss_cls: 0.5538  loss_box_reg: 0.3723  loss_rpn_cls: 0.09023  loss_rpn_loc: 0.03799  time: 0.1939  data_time: 0.0085  lr: 0.00067433  max_mem: 1522M
[32m[03/27 02:09:09 d2.utils.events]: [39m eta: 5:04:00  iter: 679  total_loss: 1.318  loss_cls: 0.7183  loss_box_reg: 0.4107  loss_rpn_cls: 0.1032  loss_rpn_loc: 0.04212  time: 0.1938  data_time: 0.0088  lr: 0.00067932  max_mem: 1522M
[32m[03/27 02:09:10 d2.utils.events]: [39m eta: 5:04:04  iter: 684  total_loss: 1.287  loss_cls: 0.6744  loss_box_reg: 0.4371  loss_rpn_cls: 0.1032  loss_rpn_loc: 0.03799  time: 0.1937  data_time: 0.0086  lr: 0.00068432  max_mem: 1522M
[32m[03/27 02:09:11 d2.utils.events]: [39m eta: 5:04:04  iter: 689  total_loss: 1.272  loss_cls: 0.649  loss_box_reg: 0.4425  loss_rpn_cls: 0.115  loss_rpn_loc: 0.04519  time: 0.1936  data_time: 0.0086  lr: 0.00068931  max_mem: 1522M
[32m[03/27 02:09:12 d2.utils.events]: [39m eta: 5:04:04  iter: 694  total_loss: 1.318  loss_cls: 0.6779  loss_box_reg: 0.479  loss_rpn_cls: 0.1201  loss_rpn_loc: 0.04653  time: 0.1935  data_time: 0.0092  lr: 0.00069431  max_mem: 1522M
[32m[03/27 02:09:13 d2.utils.events]: [39m eta: 5:04:06  iter: 699  total_loss: 1.187  loss_cls: 0.6282  loss_box_reg: 0.473  loss_rpn_cls: 0.09488  loss_rpn_loc: 0.03774  time: 0.1935  data_time: 0.0090  lr: 0.0006993  max_mem: 1522M
[32m[03/27 02:09:14 d2.utils.events]: [39m eta: 5:04:08  iter: 704  total_loss: 1.177  loss_cls: 0.6241  loss_box_reg: 0.4262  loss_rpn_cls: 0.0869  loss_rpn_loc: 0.02822  time: 0.1935  data_time: 0.0090  lr: 0.0007043  max_mem: 1522M
[32m[03/27 02:09:15 d2.utils.events]: [39m eta: 5:04:10  iter: 709  total_loss: 1.257  loss_cls: 0.6044  loss_box_reg: 0.4541  loss_rpn_cls: 0.09488  loss_rpn_loc: 0.03303  time: 0.1934  data_time: 0.0090  lr: 0.00070929  max_mem: 1522M
[32m[03/27 02:09:16 d2.utils.events]: [39m eta: 5:04:13  iter: 714  total_loss: 1.333  loss_cls: 0.6439  loss_box_reg: 0.4886  loss_rpn_cls: 0.1086  loss_rpn_loc: 0.03303  time: 0.1933  data_time: 0.0086  lr: 0.00071429  max_mem: 1522M
[32m[03/27 02:09:17 d2.utils.events]: [39m eta: 5:04:18  iter: 719  total_loss: 1.333  loss_cls: 0.6439  loss_box_reg: 0.4886  loss_rpn_cls: 0.1319  loss_rpn_loc: 0.0619  time: 0.1933  data_time: 0.0084  lr: 0.00071928  max_mem: 1522M
[32m[03/27 02:09:18 d2.utils.events]: [39m eta: 5:04:23  iter: 724  total_loss: 1.349  loss_cls: 0.6524  loss_box_reg: 0.5002  loss_rpn_cls: 0.1264  loss_rpn_loc: 0.0619  time: 0.1933  data_time: 0.0085  lr: 0.00072428  max_mem: 1522M
[32m[03/27 02:09:19 d2.utils.events]: [39m eta: 5:04:26  iter: 729  total_loss: 1.307  loss_cls: 0.6824  loss_box_reg: 0.4596  loss_rpn_cls: 0.1255  loss_rpn_loc: 0.04846  time: 0.1932  data_time: 0.0084  lr: 0.00072927  max_mem: 1522M
[32m[03/27 02:09:19 d2.utils.events]: [39m eta: 5:04:32  iter: 734  total_loss: 1.266  loss_cls: 0.5983  loss_box_reg: 0.4195  loss_rpn_cls: 0.1094  loss_rpn_loc: 0.03597  time: 0.1930  data_time: 0.0083  lr: 0.00073427  max_mem: 1522M
[32m[03/27 02:09:20 d2.utils.events]: [39m eta: 5:04:36  iter: 739  total_loss: 1.243  loss_cls: 0.616  loss_box_reg: 0.4117  loss_rpn_cls: 0.09928  loss_rpn_loc: 0.03557  time: 0.1929  data_time: 0.0083  lr: 0.00073926  max_mem: 1522M
[32m[03/27 02:09:21 d2.utils.events]: [39m eta: 5:04:40  iter: 744  total_loss: 1.172  loss_cls: 0.606  loss_box_reg: 0.408  loss_rpn_cls: 0.08329  loss_rpn_loc: 0.03557  time: 0.1928  data_time: 0.0082  lr: 0.00074426  max_mem: 1522M
[32m[03/27 02:09:22 d2.engine.hooks]: [39mOverall training speed: 746 iterations in 0:02:23 (0.1929 s / it)
[32m[03/27 02:09:22 d2.engine.hooks]: [39mTotal training time: 0:02:25 (0:00:01 on hooks)
[32m[03/27 02:09:22 d2.utils.events]: [39m eta: 5:04:40  iter: 748  total_loss: 1.185  loss_cls: 0.6094  loss_box_reg: 0.408  loss_rpn_cls: 0.07273  loss_rpn_loc: 0.03086  time: 0.1927  data_time: 0.0082  lr: 0.00074725  max_mem: 1522M
Traceback (most recent call last):
  File "train.py", line 129, in <module>
    trainer.train()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 491, in train
    super().train(self.start_iter, self.max_iter)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/defaults.py", line 501, in run_step
    self._trainer.run_step()
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 410, in run_step
    self._write_metrics(loss_dict, data_time)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 307, in _write_metrics
    SimpleTrainer.write_metrics(loss_dict, data_time, prefix)
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 321, in write_metrics
    metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
  File "/opt/ml/detection/baseline/detectron2/detectron2/engine/train_loop.py", line 321, in <dictcomp>
    metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}
KeyboardInterrupt